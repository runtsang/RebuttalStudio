{
  "projectName": "Rebuttal Studio",
  "conference": "ICLR",
  "createdAt": "2026-02-15T09:23:41.000Z",
  "updatedAt": "2026-02-28T19:58:49.249Z",
  "autosaveIntervalSeconds": 60,
  "currentStage": "First Round",
  "activeReviewerIdx": 3,
  "reviewers": [
    {
      "id": 0,
      "name": "aKxN",
      "content": "<div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">The authors present Rebuttal Studio, a five-stage LLM-assisted pipeline for writing academic peer-review responses. The platform decomposes reviewer comments into atomic issues, generates response outlines, produces draft replies, handles follow-up questions, and formats the final submission for OpenReview. The system is evaluated in a user study with 47 participants. While the practical motivation is clear, significant concerns around novelty, evaluation rigor, and reproducibility prevent a positive recommendation.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">2: fair</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">2: fair</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">4: reject, not good enough</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is unlikely, but not impossible, that you did not understand some parts of the submission or are unfamiliar with some related work.</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">Clear practical motivation: peer-review response is a genuine bottleneck for researchers at all career stages.</li><li style=\"padding: 0px;\">The five-stage pipeline design is intuitive and covers a coherent end-to-end rebuttal workflow.</li><li style=\"padding: 0px;\">The user study provides preliminary quantitative evidence (42% time reduction) that the tool is helpful.</li></ul></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">W1. Limited Novelty Over Direct LLM Use: The core technical contribution—feeding reviewer text into an LLM and structuring the output—is essentially a prompt-engineering pipeline wrapped in a UI. The paper does not demonstrate that Rebuttal Studio offers measurable advantages over a researcher directly using Claude or ChatGPT with a well-crafted prompt. A direct A/B baseline comparison is critically missing.</p><p style=\"margin: 0px 0px 0.5rem;\">W2. User Study Sample and Selection Bias: The 47-participant study relies entirely on self-reported time measurements and convenience sampling (recruited via the authors' institution and social networks). There is no control group performing the same task without the tool. Without a randomized controlled comparison, the \"42% time reduction\" claim cannot be attributed specifically to Rebuttal Studio.</p><p style=\"margin: 0px 0px 0.5rem;\">W3. Single LLM Backend—No Ablation: The paper uses a single LLM backend throughout all experiments. There is no ablation examining how different model choices (GPT-4o, Gemini 1.5 Pro, Llama 3.1) affect output quality. Given that response quality depends heavily on the underlying model, this gap limits generalizability.</p><p style=\"margin: 0px 0px 0.5rem;\">W4. No Evaluation of Downstream Impact: The paper measures user-perceived efficiency but does not evaluate whether rebuttals generated with Rebuttal Studio actually improve reviewer scores or acceptance probability. The ultimate metric of a rebuttal tool is whether it helps papers succeed at the venue.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">Q1. On Issue Grouping (Stage 1): How does the system determine whether multiple atomic issues should be grouped into a single response or addressed separately? What are the grouping heuristics, and how sensitive is final response quality to this grouping decision?</p><p style=\"margin: 0px 0px 0.5rem;\">Q2. On Prompt Transparency: Are the system prompts used for each of the five stages publicly available? The reproducibility of a prompt-engineering system depends entirely on full disclosure of the prompt design, including any few-shot examples used.</p></div></div>"
    },
    {
      "id": 1,
      "name": "mBvQ",
      "content": "<div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper introduces Rebuttal Studio, an LLM-powered tool for structuring and generating academic rebuttal letters through a five-stage pipeline. The system is evaluated in a 47-participant user study and claims a 42% reduction in rebuttal preparation time. The paper is well-written and addresses a genuine pain point, but significant concerns around academic integrity, pipeline validity, and evaluation methodology prevent me from recommending acceptance.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">2: fair</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">2: fair</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">4: reject, not good enough</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">4: You are confident in your assessment, but not absolutely certain. It is unlikely you would change your mind after discussing with other reviewers.</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">Strong practical motivation; rebuttal writing is a high-stakes, time-intensive task for researchers.</li><li style=\"padding: 0px;\">The pipeline design is principled and reflects how experienced authors actually structure rebuttal arguments.</li><li style=\"padding: 0px;\">The open-source commitment increases community value and enables future reproducible research on this tool.</li></ul></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">W1. Ethical Concerns Around AI-Generated Academic Discourse: Generating AI-assisted rebuttals raises serious questions about academic integrity. If reviewers cannot distinguish author-written from AI-generated responses, the scientific peer review process may be undermined. The paper does not discuss disclosure requirements, venue-specific policies on AI use in rebuttals, or how the tool should be used responsibly. This is not a minor omission—it is central to whether the tool should exist at all.</p><p style=\"margin: 0px 0px 0.5rem;\">W2. Missing Ablation of Pipeline Stages: The paper presents a five-stage system but does not ablate individual components. Is atomic issue decomposition (Stage 1) necessary, or does it add overhead without improving response coverage? Does outline generation (Stage 2) improve over directly drafting (Stage 3)? Without ablation, it is unclear which stages contribute meaningfully to reported improvements.</p><p style=\"margin: 0px 0px 0.5rem;\">W3. Evaluation Metrics Do Not Measure Response Quality: The primary metrics are (a) time savings and (b) self-reported completeness ratings. Neither metric directly measures the quality of the actual rebuttal text. The paper should include an expert evaluation (e.g., having PC members blind-rate responses produced with and without the tool) to validate quality claims.</p><p style=\"margin: 0px 0px 0.5rem;\">W4. Scope Is Effectively ICLR-Only: The system's scoring rubrics and section labels (Soundness, Presentation, Contribution) are hardcoded to ICLR format. The paper does not demonstrate how the system would adapt to ACL (which uses different criteria), NeurIPS, CVPR, or journal-format reviews that can span 3–5 pages.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">Q1. On Venue Policy Compliance: Have the authors surveyed how major venues (ICLR, NeurIPS, ACL, CVPR) currently treat AI-assisted rebuttals in their author guidelines? Does the paper plan to add a disclosure mechanism—for instance, automatically appending a statement indicating AI assistance was used?</p><p style=\"margin: 0px 0px 0.5rem;\">Q2. On Stage 3 Style Control: The paper mentions 'style control' as a feature. What specific style parameters are controllable, and how are they implemented? Is the style encoding symbolic (e.g., formal/semi-formal), prompt-based, or learned from user-provided examples? How often do users override the system's default style selections?</p></div></div>"
    },
    {
      "id": 2,
      "name": "TvpR",
      "content": "<div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">The authors present Rebuttal Studio, a web application that implements a structured, multi-stage workflow for generating academic rebuttal letters with LLM assistance. The system is motivated by the difficulty researchers face when managing feedback from multiple reviewers under tight deadlines. Evaluated on 47 users with real rebuttal tasks, the paper reports improvements in time efficiency and response coverage. The work addresses a real and important problem, and the five-stage design is principled. However, several concerns about evaluation methodology, component validation, and follow-up handling prevent a strong recommendation at this point.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">2: fair</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">6: marginally above acceptance threshold</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is unlikely, but not impossible, that you did not understand some parts of the submission or are unfamiliar with some related work.</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The five-stage pipeline is well-motivated and covers a coherent workflow from parsing to final submission formatting.</li><li style=\"padding: 0px;\">Atomic issue decomposition (Stage 1) is a particularly valuable step: it enforces comprehensive coverage and prevents the common mistake of addressing multiple reviewer concerns in an ambiguous, umbrella response.</li><li style=\"padding: 0px;\">OpenReview-specific LaTeX color rendering and blockquote formatting in Stage 3 are practical contributions that reflect real submission requirements.</li><li style=\"padding: 0px;\">The paper is well-written, the figures are informative, and the system walkthrough is easy to follow.</li></ul></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">W1. Atomic Decomposition Quality Not Validated: Stage 1 decomposes reviewer comments into atomic issues, a critical step that determines all downstream response quality. The paper does not evaluate the accuracy or completeness of this decomposition against human expert annotation. If the system merges distinct concerns or fragments a single concern into redundant issues, subsequent stages will produce low-quality responses regardless of LLM capability. A small inter-rater agreement study (e.g., κ ≥ 0.7) would provide essential validation.</p><p style=\"margin: 0px 0px 0.5rem;\">W2. Follow-Up Question Handling (Stage 4) Is Underspecified and Underevaluated: Stage 4, which handles reviewer follow-up questions during the rebuttal discussion period, receives minimal description in the paper (roughly one paragraph). In practice, follow-up handling is often the most impactful phase of a rebuttal—it represents active dialogue with reviewers who may be wavering. The paper should describe how Stage 4 grounds responses in prior rebuttal context, how it retrieves relevant prior Stage 3 drafts, and how it was evaluated.</p><p style=\"margin: 0px 0px 0.5rem;\">W3. Multi-Reviewer Coordination (Stage 5) Not Specifically Evaluated: The final stage generates a unified meta-reviewer response template. However, the user study evaluates only overall satisfaction and time, not specifically the Stage 5 output quality. In practice, the area chair reads all responses together and inconsistencies across individual reviewer responses can hurt acceptance probability.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">Q1. On Scalability to Long Reviews: Some venue reviews (e.g., TPAMI, JMLR) can contain 20 or more distinct concerns spanning several pages. How does the atomic decomposition handle very long reviews, and does performance degrade as input context grows? Is there a token limit that caps the system's effectiveness on longer reviews?</p><p style=\"margin: 0px 0px 0.5rem;\">Q2. On Tone Calibration Mechanism: The paper mentions a 'tone management' feature in Stage 3. How is tone operationalized—through prompt engineering, user-provided example sentences, or a learned style model? Have users found the default tone appropriate across different cultural and linguistic backgrounds, and how frequently do they override it?</p><p style=\"margin: 0px 0px 0.5rem;\">Q3. On LLM Hallucination Detection: When the underlying LLM generates a factually incorrect response (e.g., inventing experimental results, misattributing citations, or generating numbers inconsistent with the actual paper), how does Rebuttal Studio detect and flag this? Are there any automated guardrails, or is fact-checking entirely the user's responsibility?</p></div></div>"
    },
    {
      "id": 3,
      "name": "wLhF",
      "content": "<div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">The paper presents Rebuttal Studio, an end-to-end AI-assisted platform for writing academic peer-review responses through a five-stage pipeline: (1) comment parsing and atomic issue decomposition, (2) response outline generation, (3) draft writing with style control, (4) follow-up handling, and (5) final formatting. Evaluated in a 47-participant user study and a real-world deployment processing 2,000+ rebuttals, the paper provides strong evidence of practical impact. This is a well-motivated, carefully engineered systems contribution that addresses a genuine bottleneck in academic research. I recommend acceptance.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">4: excellent</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">8: accept, good paper</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is unlikely, but not impossible, that you did not understand some parts of the submission or are unfamiliar with some related work.</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">Addresses a genuine and underserved need: rebuttal writing is high-stakes, time-intensive, and there are currently no purpose-built tools for this workflow.</li><li style=\"padding: 0px;\">The five-stage pipeline decomposition is principled and reflects the actual cognitive stages researchers go through—decomposing concerns, planning arguments, drafting, handling follow-ups, and formatting.</li><li style=\"padding: 0px;\">Atomic issue decomposition is a novel and practically important contribution: it enforces comprehensive coverage and prevents vague umbrella responses that fail to satisfy reviewers.</li><li style=\"padding: 0px;\">Real-world deployment data (2,000+ rebuttals processed) provides strong evidence of adoption beyond the controlled study and validates practical utility.</li><li style=\"padding: 0px;\">OpenReview-specific LaTeX color rendering (Stage 3) and meta-reviewer template (Stage 5) show attention to real submission mechanics that most researchers find tedious and error-prone.</li></ul></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">W1. No Randomized Control Condition: The user study measures time within subjects but lacks a between-subjects comparison against a control group using a direct LLM interface. While the reported 42% time reduction is promising, a controlled experiment would substantially strengthen the causal claim that Rebuttal Studio—rather than LLM access in general—drives the improvement.</p><p style=\"margin: 0px 0px 0.5rem;\">W2. Long-Term Behavioral Effects Not Studied: The paper does not discuss whether extended use of Rebuttal Studio might cause researchers to become over-reliant on AI-generated arguments, gradually eroding their ability to independently articulate their research contributions. This is an important question for any tool designed to assist—rather than replace—expert judgment.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">Q1. On Data Privacy: The platform processes potentially sensitive content—unpublished results, confidential review text that may be under embargo, and proprietary research details. What are the server-side data retention policies? Are review contents transmitted to third-party LLM APIs, and if so, what contractual safeguards exist for data privacy and confidentiality?</p><p style=\"margin: 0px 0px 0.5rem;\">Q2. On Cross-Venue Generalization: The evaluation focuses on ICLR. Are there plans to extend the system to handle ACL (which uses different criteria and longer reviews), NeurIPS, CVPR, or journal-format reviews? If so, what changes to Stage 1 (decomposition) and Stage 5 (unified template) would be required, and are these differences superficial (format) or fundamental (workflow)?</p></div></div>"
    }
  ],
  "breakdownData": {
    "0": {
      "scores": {
        "rating": "4",
        "confidence": "3",
        "soundness": "2",
        "presentation": "3",
        "contribution": "2"
      },
      "sections": {
        "summary": "The authors present Rebuttal Studio, a five-stage LLM-assisted pipeline for writing academic peer-review responses. The platform decomposes reviewer comments into atomic issues, generates response outlines, produces draft replies, handles follow-up questions, and formats the final submission for OpenReview. The system is evaluated in a user study with 47 participants. While the practical motivation is clear, significant concerns around novelty, evaluation rigor, and reproducibility prevent a positive recommendation.",
        "strength": "Clear practical motivation: peer-review response is a genuine bottleneck for researchers at all career stages.\nThe five-stage pipeline design is intuitive and covers a coherent end-to-end rebuttal workflow.\nThe user study provides preliminary quantitative evidence (42% time reduction) that the tool is helpful.",
        "weakness": "W1. Limited Novelty Over Direct LLM Use: The core technical contribution—feeding reviewer text into an LLM and structuring the output—is essentially a prompt-engineering pipeline wrapped in a UI. The paper does not demonstrate that Rebuttal Studio offers measurable advantages over a researcher directly using Claude or ChatGPT with a well-crafted prompt. A direct A/B baseline comparison is critically missing.\n\nW2. User Study Sample and Selection Bias: The 47-participant study relies entirely on self-reported time measurements and convenience sampling (recruited via the authors' institution and social networks). There is no control group performing the same task without the tool. Without a randomized controlled comparison, the \"42% time reduction\" claim cannot be attributed specifically to Rebuttal Studio.\n\nW3. Single LLM Backend—No Ablation: The paper uses a single LLM backend throughout all experiments. There is no ablation examining how different model choices (GPT-4o, Gemini 1.5 Pro, Llama 3.1) affect output quality. Given that response quality depends heavily on the underlying model, this gap limits generalizability.\n\nW4. No Evaluation of Downstream Impact: The paper measures user-perceived efficiency but does not evaluate whether rebuttals generated with Rebuttal Studio actually improve reviewer scores or acceptance probability. The ultimate metric of a rebuttal tool is whether it helps papers succeed at the venue.",
        "questions": "Q1. On Issue Grouping (Stage 1): How does the system determine whether multiple atomic issues should be grouped into a single response or addressed separately? What are the grouping heuristics, and how sensitive is final response quality to this grouping decision?\n\nQ2. On Prompt Transparency: Are the system prompts used for each of the five stages publicly available? The reproducibility of a prompt-engineering system depends entirely on full disclosure of the prompt design, including any few-shot examples used."
      },
      "atomicIssues": [
        {
          "id": "weakness1",
          "source": "weakness",
          "text": "Limited Novelty Over Direct LLM Use: The core technical contribution is essentially a prompt-engineering pipeline wrapped in a UI, and no direct A/B comparison against raw LLM use is provided."
        },
        {
          "id": "weakness2",
          "source": "weakness",
          "text": "User Study Sample and Selection Bias: 47-participant convenience sample with no randomized control group; the 42% time-reduction claim cannot be causally attributed to the tool."
        },
        {
          "id": "weakness3",
          "source": "weakness",
          "text": "Single LLM Backend—No Ablation: Only one LLM (Claude) was tested; no comparison across GPT-4o, Gemini, or open-source alternatives limits generalizability."
        },
        {
          "id": "weakness4",
          "source": "weakness",
          "text": "No Evaluation of Downstream Impact: The paper measures efficiency but not whether rebuttals generated by the tool actually improve reviewer scores or paper acceptance rates."
        },
        {
          "id": "question1",
          "source": "question",
          "text": "Issue Grouping Heuristics (Stage 1): How does the system decide to group or separate atomic issues, and how sensitive is downstream quality to this grouping decision?"
        },
        {
          "id": "question2",
          "source": "question",
          "text": "Prompt Transparency: Are the system prompts for each of the five stages publicly available, including any few-shot examples? Full prompt disclosure is necessary for reproducibility."
        }
      ],
      "responses": [
        {
          "id": "Response1",
          "title": "Limited Novelty vs. Direct LLM Use",
          "source": "weakness",
          "source_id": "weakness1",
          "quoted_issue": "The core technical contribution—feeding reviewer text into an LLM and structuring the output—is essentially a prompt-engineering pipeline wrapped in a UI. No direct A/B comparison against raw LLM use is provided."
        },
        {
          "id": "Response2",
          "title": "User Study Validity and Control Condition",
          "source": "weakness",
          "source_id": "weakness2",
          "quoted_issue": "47-participant convenience sample with no randomized control group; the 42% time-reduction claim cannot be causally attributed to the tool."
        },
        {
          "id": "Response3",
          "title": "Multi-LLM Ablation Study",
          "source": "weakness",
          "source_id": "weakness3",
          "quoted_issue": "Only one LLM (Claude) was tested; no comparison across GPT-4o, Gemini, or open-source alternatives limits generalizability."
        },
        {
          "id": "Response4",
          "title": "Downstream Rebuttal Effectiveness",
          "source": "weakness",
          "source_id": "weakness4",
          "quoted_issue": "The paper measures efficiency but not whether rebuttals generated by the tool actually improve reviewer scores or paper acceptance rates."
        },
        {
          "id": "Response5",
          "title": "Issue Grouping Heuristics",
          "source": "question",
          "source_id": "question1",
          "quoted_issue": "How does the system decide to group or separate atomic issues, and how sensitive is downstream quality to this grouping decision?"
        },
        {
          "id": "Response6",
          "title": "Prompt Transparency and Reproducibility",
          "source": "question",
          "source_id": "question2",
          "quoted_issue": "Are the system prompts for each of the five stages publicly available, including any few-shot examples?"
        }
      ]
    },
    "1": {
      "scores": {
        "rating": "4",
        "confidence": "4",
        "soundness": "2",
        "presentation": "3",
        "contribution": "2"
      },
      "sections": {
        "summary": "This paper introduces Rebuttal Studio, an LLM-powered tool for structuring and generating academic rebuttal letters through a five-stage pipeline. The system is evaluated in a 47-participant user study and claims a 42% reduction in rebuttal preparation time. The paper is well-written and addresses a genuine pain point, but significant concerns around academic integrity, pipeline validity, and evaluation methodology prevent me from recommending acceptance.",
        "strength": "Strong practical motivation; rebuttal writing is a high-stakes, time-intensive task for researchers.\nThe pipeline design is principled and reflects how experienced authors actually structure rebuttal arguments.\nThe open-source commitment increases community value and enables future reproducible research on this tool.",
        "weakness": "W1. Ethical Concerns Around AI-Generated Academic Discourse: Generating AI-assisted rebuttals raises serious questions about academic integrity. If reviewers cannot distinguish author-written from AI-generated responses, the scientific peer review process may be undermined. The paper does not discuss disclosure requirements, venue-specific policies on AI use in rebuttals, or how the tool should be used responsibly. This is not a minor omission—it is central to whether the tool should exist at all.\n\nW2. Missing Ablation of Pipeline Stages: The paper presents a five-stage system but does not ablate individual components. Is atomic issue decomposition (Stage 1) necessary, or does it add overhead without improving response coverage? Does outline generation (Stage 2) improve over directly drafting (Stage 3)? Without ablation, it is unclear which stages contribute meaningfully to reported improvements.\n\nW3. Evaluation Metrics Do Not Measure Response Quality: The primary metrics are (a) time savings and (b) self-reported completeness ratings. Neither metric directly measures the quality of the actual rebuttal text. The paper should include an expert evaluation (e.g., having PC members blind-rate responses produced with and without the tool) to validate quality claims.\n\nW4. Scope Is Effectively ICLR-Only: The system's scoring rubrics and section labels (Soundness, Presentation, Contribution) are hardcoded to ICLR format. The paper does not demonstrate how the system would adapt to ACL (which uses different criteria), NeurIPS, CVPR, or journal-format reviews that can span 3–5 pages.",
        "questions": "Q1. On Venue Policy Compliance: Have the authors surveyed how major venues (ICLR, NeurIPS, ACL, CVPR) currently treat AI-assisted rebuttals in their author guidelines? Does the paper plan to add a disclosure mechanism—for instance, automatically appending a statement indicating AI assistance was used?\n\nQ2. On Stage 3 Style Control: The paper mentions 'style control' as a feature. What specific style parameters are controllable, and how are they implemented? Is the style encoding symbolic (e.g., formal/semi-formal), prompt-based, or learned from user-provided examples? How often do users override the system's default style selections?"
      },
      "atomicIssues": [
        {
          "id": "weakness1",
          "source": "weakness",
          "text": "Ethical Concerns: AI-assisted rebuttals raise academic integrity issues; the paper lacks discussion of disclosure requirements and venue policies on AI use in peer-review responses."
        },
        {
          "id": "weakness2",
          "source": "weakness",
          "text": "Missing Pipeline Ablation: No ablation study isolates which of the five stages (decomposition, outline, draft, follow-up, formatting) drive the reported improvements."
        },
        {
          "id": "weakness3",
          "source": "weakness",
          "text": "Evaluation Metrics Don't Measure Quality: Time savings and self-reported completeness do not measure rebuttal text quality; expert blind evaluation is needed."
        },
        {
          "id": "weakness4",
          "source": "weakness",
          "text": "ICLR-Only Scope: Scoring rubrics and section labels are hardcoded to ICLR format; generalization to ACL, NeurIPS, CVPR, or journal reviews is not demonstrated."
        },
        {
          "id": "question1",
          "source": "question",
          "text": "Venue Policy Compliance: Have major venues been surveyed on AI-assisted rebuttal policies, and will the tool include an automatic disclosure mechanism?"
        },
        {
          "id": "question2",
          "source": "question",
          "text": "Stage 3 Style Control Implementation: What style parameters are controllable and how are they implemented (symbolic, prompt-based, or learned)?"
        }
      ],
      "responses": [
        {
          "id": "Response1",
          "title": "Ethical Concerns and Responsible Use Policy",
          "source": "weakness",
          "source_id": "weakness1",
          "quoted_issue": "AI-assisted rebuttals raise academic integrity issues; the paper lacks discussion of disclosure requirements and venue policies on AI use in peer-review responses."
        },
        {
          "id": "Response2",
          "title": "Per-Stage Ablation Study",
          "source": "weakness",
          "source_id": "weakness2",
          "quoted_issue": "No ablation study isolates which of the five stages drive the reported improvements."
        },
        {
          "id": "Response3",
          "title": "Expert Blind Evaluation of Response Quality",
          "source": "weakness",
          "source_id": "weakness3",
          "quoted_issue": "Time savings and self-reported completeness do not measure rebuttal text quality; expert blind evaluation is needed."
        },
        {
          "id": "Response4",
          "title": "Multi-Venue Format Support",
          "source": "weakness",
          "source_id": "weakness4",
          "quoted_issue": "Scoring rubrics and section labels are hardcoded to ICLR format; generalization to ACL, NeurIPS, CVPR, or journal reviews is not demonstrated."
        },
        {
          "id": "Response5",
          "title": "Venue Policy Survey and Disclosure Mechanism",
          "source": "question",
          "source_id": "question1",
          "quoted_issue": "Have major venues been surveyed on AI-assisted rebuttal policies, and will the tool include an automatic disclosure mechanism?"
        },
        {
          "id": "Response6",
          "title": "Style Control Technical Details",
          "source": "question",
          "source_id": "question2",
          "quoted_issue": "What style parameters are controllable and how are they implemented?"
        }
      ]
    },
    "2": {
      "scores": {
        "rating": "6",
        "confidence": "3",
        "soundness": "3",
        "presentation": "3",
        "contribution": "2"
      },
      "sections": {
        "summary": "The authors present Rebuttal Studio, a web application that implements a structured, multi-stage workflow for generating academic rebuttal letters with LLM assistance. The system is motivated by the difficulty researchers face when managing feedback from multiple reviewers under tight deadlines. Evaluated on 47 users with real rebuttal tasks, the paper reports improvements in time efficiency and response coverage. The work addresses a real and important problem, and the five-stage design is principled. However, several concerns about evaluation methodology, component validation, and follow-up handling prevent a strong recommendation at this point.",
        "strength": "The five-stage pipeline is well-motivated and covers a coherent workflow from parsing to final submission formatting.\nAtomic issue decomposition (Stage 1) is a particularly valuable step: it enforces comprehensive coverage and prevents the common mistake of addressing multiple reviewer concerns in an ambiguous, umbrella response.\nOpenReview-specific LaTeX color rendering and blockquote formatting in Stage 3 are practical contributions that reflect real submission requirements.\nThe paper is well-written, the figures are informative, and the system walkthrough is easy to follow.",
        "weakness": "W1. Atomic Decomposition Quality Not Validated: Stage 1 decomposes reviewer comments into atomic issues, a critical step that determines all downstream response quality. The paper does not evaluate the accuracy or completeness of this decomposition against human expert annotation. If the system merges distinct concerns or fragments a single concern into redundant issues, subsequent stages will produce low-quality responses regardless of LLM capability. A small inter-rater agreement study (e.g., κ ≥ 0.7) would provide essential validation.\n\nW2. Follow-Up Question Handling (Stage 4) Is Underspecified and Underevaluated: Stage 4, which handles reviewer follow-up questions during the rebuttal discussion period, receives minimal description in the paper (roughly one paragraph). In practice, follow-up handling is often the most impactful phase of a rebuttal—it represents active dialogue with reviewers who may be wavering. The paper should describe how Stage 4 grounds responses in prior rebuttal context, how it retrieves relevant prior Stage 3 drafts, and how it was evaluated.\n\nW3. Multi-Reviewer Coordination (Stage 5) Not Specifically Evaluated: The final stage generates a unified meta-reviewer response template. However, the user study evaluates only overall satisfaction and time, not specifically the Stage 5 output quality. In practice, the area chair reads all responses together and inconsistencies across individual reviewer responses can hurt acceptance probability.",
        "questions": "Q1. On Scalability to Long Reviews: Some venue reviews (e.g., TPAMI, JMLR) can contain 20 or more distinct concerns spanning several pages. How does the atomic decomposition handle very long reviews, and does performance degrade as input context grows? Is there a token limit that caps the system's effectiveness on longer reviews?\n\nQ2. On Tone Calibration Mechanism: The paper mentions a 'tone management' feature in Stage 3. How is tone operationalized—through prompt engineering, user-provided example sentences, or a learned style model? Have users found the default tone appropriate across different cultural and linguistic backgrounds, and how frequently do they override it?\n\nQ3. On LLM Hallucination Detection: When the underlying LLM generates a factually incorrect response (e.g., inventing experimental results, misattributing citations, or generating numbers inconsistent with the actual paper), how does Rebuttal Studio detect and flag this? Are there any automated guardrails, or is fact-checking entirely the user's responsibility?"
      },
      "atomicIssues": [
        {
          "id": "weakness1",
          "source": "weakness",
          "text": "Atomic Decomposition Not Validated: Stage 1's accuracy and completeness are not evaluated against human annotation; an inter-rater agreement study (e.g., κ) is missing."
        },
        {
          "id": "weakness2",
          "source": "weakness",
          "text": "Stage 4 (Follow-Up Handling) Underspecified: The most critical rebuttal phase—live reviewer dialogue—receives only one paragraph of description and no dedicated evaluation."
        },
        {
          "id": "weakness3",
          "source": "weakness",
          "text": "Stage 5 (Multi-Reviewer Coordination) Not Specifically Evaluated: The unified meta-reviewer template is not evaluated for consistency across individual reviewer responses."
        },
        {
          "id": "question1",
          "source": "question",
          "text": "Scalability to Long Reviews: How does the atomic decomposition perform on 20+ concern reviews (e.g., TPAMI, JMLR) and does performance degrade near token limits?"
        },
        {
          "id": "question2",
          "source": "question",
          "text": "Tone Calibration Mechanism: How is tone operationalized—prompt engineering, user examples, or a learned model? How often do users override the default?"
        },
        {
          "id": "question3",
          "source": "question",
          "text": "LLM Hallucination Detection: When the LLM generates factually incorrect rebuttal text (invented numbers, misattributed citations), how does Rebuttal Studio detect and flag this?"
        }
      ],
      "responses": [
        {
          "id": "Response1",
          "title": "Atomic Decomposition Validation Study",
          "source": "weakness",
          "source_id": "weakness1",
          "quoted_issue": "Stage 1's accuracy and completeness are not evaluated against human annotation; an inter-rater agreement study is missing."
        },
        {
          "id": "Response2",
          "title": "Stage 4 Follow-Up Handling Design",
          "source": "weakness",
          "source_id": "weakness2",
          "quoted_issue": "The most critical rebuttal phase—live reviewer dialogue—receives only one paragraph of description and no dedicated evaluation."
        },
        {
          "id": "Response3",
          "title": "Stage 5 Multi-Reviewer Consistency",
          "source": "weakness",
          "source_id": "weakness3",
          "quoted_issue": "The unified meta-reviewer template is not evaluated for consistency across individual reviewer responses."
        },
        {
          "id": "Response4",
          "title": "Scalability to Long Reviews",
          "source": "question",
          "source_id": "question1",
          "quoted_issue": "How does the atomic decomposition perform on 20+ concern reviews and does performance degrade near token limits?"
        },
        {
          "id": "Response5",
          "title": "Tone Calibration Technical Details",
          "source": "question",
          "source_id": "question2",
          "quoted_issue": "How is tone operationalized and how often do users override the default?"
        },
        {
          "id": "Response6",
          "title": "Hallucination Detection and Guardrails",
          "source": "question",
          "source_id": "question3",
          "quoted_issue": "When the LLM generates factually incorrect rebuttal text, how does Rebuttal Studio detect and flag this?"
        }
      ]
    },
    "3": {
      "scores": {
        "rating": "8",
        "confidence": "3",
        "soundness": "3",
        "presentation": "4",
        "contribution": "3"
      },
      "sections": {
        "summary": "The paper presents Rebuttal Studio, an end-to-end AI-assisted platform for writing academic peer-review responses through a five-stage pipeline: (1) comment parsing and atomic issue decomposition, (2) response outline generation, (3) draft writing with style control, (4) follow-up handling, and (5) final formatting. Evaluated in a 47-participant user study and a real-world deployment processing 2,000+ rebuttals, the paper provides strong evidence of practical impact. This is a well-motivated, carefully engineered systems contribution that addresses a genuine bottleneck in academic research. I recommend acceptance.",
        "strength": "Addresses a genuine and underserved need: rebuttal writing is high-stakes, time-intensive, and there are currently no purpose-built tools for this workflow.\nThe five-stage pipeline decomposition is principled and reflects the actual cognitive stages researchers go through—decomposing concerns, planning arguments, drafting, handling follow-ups, and formatting.\nAtomic issue decomposition is a novel and practically important contribution: it enforces comprehensive coverage and prevents vague umbrella responses that fail to satisfy reviewers.\nReal-world deployment data (2,000+ rebuttals processed) provides strong evidence of adoption beyond the controlled study and validates practical utility.\nOpenReview-specific LaTeX color rendering (Stage 3) and meta-reviewer template (Stage 5) show attention to real submission mechanics that most researchers find tedious and error-prone.",
        "weakness": "W1. No Randomized Control Condition: The user study measures time within subjects but lacks a between-subjects comparison against a control group using a direct LLM interface. While the reported 42% time reduction is promising, a controlled experiment would substantially strengthen the causal claim that Rebuttal Studio—rather than LLM access in general—drives the improvement.\n\nW2. Long-Term Behavioral Effects Not Studied: The paper does not discuss whether extended use of Rebuttal Studio might cause researchers to become over-reliant on AI-generated arguments, gradually eroding their ability to independently articulate their research contributions. This is an important question for any tool designed to assist—rather than replace—expert judgment.",
        "questions": "Q1. On Data Privacy: The platform processes potentially sensitive content—unpublished results, confidential review text that may be under embargo, and proprietary research details. What are the server-side data retention policies? Are review contents transmitted to third-party LLM APIs, and if so, what contractual safeguards exist for data privacy and confidentiality?\n\nQ2. On Cross-Venue Generalization: The evaluation focuses on ICLR. Are there plans to extend the system to handle ACL (which uses different criteria and longer reviews), NeurIPS, CVPR, or journal-format reviews? If so, what changes to Stage 1 (decomposition) and Stage 5 (unified template) would be required, and are these differences superficial (format) or fundamental (workflow)?"
      },
      "atomicIssues": [
        {
          "id": "weakness1",
          "source": "weakness",
          "text": "No Randomized Control Condition: The user study lacks a between-subjects comparison against a control group using direct LLM access; the causal claim for 42% time reduction is not fully supported."
        },
        {
          "id": "weakness2",
          "source": "weakness",
          "text": "Long-Term Behavioral Effects Unstudied: No investigation of whether extended use causes over-reliance, eroding researchers' independent ability to articulate their contributions."
        },
        {
          "id": "question1",
          "source": "question",
          "text": "Data Privacy and Retention Policies: What are the server-side data retention policies for processed rebuttals? Are review contents transmitted to third-party LLM APIs?"
        },
        {
          "id": "question2",
          "source": "question",
          "text": "Cross-Venue Generalization Plan: Are there concrete plans for ACL, NeurIPS, CVPR, and journal-format support? What changes to Stages 1 and 5 would be required?"
        }
      ],
      "responses": [
        {
          "id": "Response1",
          "title": "Strengthening the Control Condition",
          "source": "weakness",
          "source_id": "weakness1",
          "quoted_issue": "The user study lacks a between-subjects comparison against a control group using direct LLM access."
        },
        {
          "id": "Response2",
          "title": "Long-Term Behavioral Effects",
          "source": "weakness",
          "source_id": "weakness2",
          "quoted_issue": "No investigation of whether extended use causes over-reliance, eroding researchers' independent ability to articulate contributions."
        },
        {
          "id": "Response3",
          "title": "Data Privacy and Security Architecture",
          "source": "question",
          "source_id": "question1",
          "quoted_issue": "What are the server-side data retention policies? Are review contents transmitted to third-party LLM APIs?"
        },
        {
          "id": "Response4",
          "title": "Cross-Venue Roadmap",
          "source": "question",
          "source_id": "question2",
          "quoted_issue": "Are there concrete plans for ACL, NeurIPS, CVPR, and journal-format support?"
        }
      ]
    }
  },
  "stage2Replies": {
    "0": {
      "Response1": {
        "outline": "key distinction from raw LLM use → 5-stage scaffolding enforces systematic decomposition + coverage → raw LLM users skip decomposition step → miss minor concerns → spend extra time reformatting.\n\nnew controlled study (N=15, within-subjects) → Rebuttal Studio 23% faster, 31% more atomic issues addressed → statistical significance p=0.031.\n\ncore value-add = workflow enforcement, not LLM capability → structured prompting + context handoff across stages → not replicable by ad-hoc ChatGPT use.",
        "draft": "We thank the reviewer for this important point. To address the missing baseline, we have conducted a within-subjects comparison study (N=15) where participants completed two equivalent rebuttal tasks—one using Rebuttal Studio and one using direct ChatGPT access with no structured guidance. Rebuttal Studio users completed tasks 23% faster (p=0.031, paired t-test) and addressed 31% more atomic reviewer issues (assessed by three independent annotators, κ=0.81). The key advantage lies not in LLM capability per se, but in workflow scaffolding: the five-stage structure enforces systematic issue decomposition and coverage that unstructured LLM sessions do not guarantee. In the control condition, participants frequently overlooked minor reviewer points and spent significant time manually reformatting outputs for OpenReview. We will add this comparison as a controlled baseline experiment in the revised paper.",
        "assets": []
      },
      "Response2": {
        "outline": "acknowledge convenience sampling limitation → clarify within-subjects design provides internal control → explain that N=47 is standard for HCI/systems papers (cite CHI venue norms) → commit to reporting participant demographics (career stage, venue experience) in appendix → distinguish from medical RCT requirements → plan future large-scale deployment study.",
        "draft": "We acknowledge this limitation. We clarify that our study uses a within-subjects design: each participant completed the rebuttal task both with and without the tool (counterbalanced), providing an internal control that partially mitigates selection bias. The N=47 sample is consistent with systems evaluation papers at HCI venues (median N=24 in CHI 2024 user studies), and we report detailed demographic breakdowns in Appendix B, including career stage (PhD: 62%, postdoc: 21%, faculty: 17%) and prior rebuttal experience. We agree that a large-scale deployment study with pre-registered hypotheses would strengthen causal claims, and we plan this as a six-month follow-up study using real venue submissions from consenting authors.",
        "assets": []
      },
      "Response3": {
        "outline": "report preliminary ablation across Claude 3.5 Sonnet, GPT-4o, Gemini 1.5 Pro, Llama 3.1 70B → expert evaluation (5 PC members blind-rate 20 drafts per model) → all models score within 0.3 points on 5-point scale → modular design allows easy backend swapping via API key → plan to add configurable backend selection in v1.1.",
        "draft": "We have conducted a preliminary backend ablation study. Five PC members from top ML venues blind-rated 20 randomly sampled Stage 3 drafts produced by four LLM backends: Claude 3.5 Sonnet, GPT-4o, Gemini 1.5 Pro, and Llama 3.1 70B. Average expert quality ratings (1–5 scale) were 4.2, 4.0, 3.9, and 3.7 respectively. The differences were not statistically significant (Kruskal-Wallis, p=0.18), suggesting the five-stage pipeline structure is the primary driver of quality rather than the specific LLM. The system's modular architecture allows backend swapping via a single API-key configuration, and we will add user-selectable backend support in our v1.1 release. We will include this ablation in the revised paper.",
        "assets": []
      },
      "Response4": {
        "outline": "acknowledge difficulty of measuring acceptance rate → explain that running RCT across real submissions is infeasible in a single-year deployment → cite proxy evidence: reviewer score change after rebuttal correlates with acceptance (ref: ICLR analysis papers) → propose feasible proxy: reviewer final-round confidence delta → plan 12-month follow-up collecting post-decision data from consenting users.",
        "draft": "We acknowledge this limitation and agree it represents an important open question. Measuring downstream acceptance rates would require a randomized experiment across real venue submissions—a design that is infeasible within a single paper's timeline. As a proxy, we plan to collect reviewer confidence-score deltas (initial vs. final rating) from consenting users in our ongoing deployment study. Prior work on the ICLR review process has shown that rebuttal-driven confidence increases are strongly correlated with final acceptance decisions (Gao et al., 2024). We will add a 12-month deployment follow-up study plan as future work in the revised paper.",
        "assets": []
      },
      "Response5": {
        "outline": "explain grouping algorithm: semantic cosine similarity (threshold 0.75) + shared source label (weakness/question) → two issues merged if they address the same underlying concern → show worked examples: 'novelty' issues merged, 'evaluation' issues merged, but 'novelty' and 'evaluation' kept separate → ablation: grouped vs. ungrouped responses rated by 3 annotators → grouped: 4.1/5, ungrouped: 3.4/5.",
        "draft": "The grouping algorithm in Stage 1 operates in two steps. First, we compute pairwise semantic similarity between atomic issues using sentence-BERT embeddings; issue pairs with cosine similarity ≥ 0.75 are candidate merge pairs. Second, we apply a shared-concern constraint: only issues from the same source category (all weaknesses, or all questions) and sharing a named concern (e.g., 'novelty', 'evaluation') are merged. For example, two weakness statements both questioning novelty relative to prior work would be merged into a single response, but a novelty weakness and an evaluation weakness remain separate. In an ablation study (N=30 responses, rated by 3 annotators), grouped responses scored 4.1/5 vs. 3.4/5 for ungrouped, confirming that appropriate merging improves coherence without losing coverage. We will add this analysis to Section 3.1 of the revised paper.",
        "assets": []
      },
      "Response6": {
        "outline": "commit to full prompt release in appendix + GitHub → describe modular prompt design: one system prompt per stage → few-shot examples: 3 examples per stage drawn from anonymized real rebuttals (with author consent) → prompt versioning tracked in git alongside codebase → release as part of open-source v1.0.",
        "draft": "We fully agree that prompt transparency is essential for reproducibility. All system prompts used in each of the five stages will be released in Appendix C of the revised paper, along with the three few-shot examples per stage drawn from anonymized real rebuttals (with explicit author consent). Prompts are version-controlled alongside the main codebase and tagged per release. We will add prompt release to our open-source v1.0 repository at [anonymized URL for review]. This will allow other researchers to replicate our results and adapt the prompts for venue-specific requirements.",
        "assets": []
      }
    },
    "1": {
      "Response1": {
        "outline": "acknowledge the ethical dimension directly and seriously → cite emerging venue policies: ICLR 2026 author guide does not prohibit AI assistance but recommends disclosure → propose a disclosure feature: auto-generated statement appended to rebuttal → frame tool as assistive (like Grammarly) not ghostwriting → add dedicated ethics section to paper.",
        "draft": "We take this concern very seriously. We have surveyed AI-assistance policies across major venues: as of Feb 2026, ICLR, NeurIPS, and ACL do not prohibit AI assistance in rebuttals but recommend transparency where used. We will add a disclosure mechanism to the v1.1 release: a configurable statement (e.g., 'This rebuttal was prepared with AI-assisted drafting via Rebuttal Studio; all arguments were verified and edited by the authors.') that users can optionally append to their submission. We frame Rebuttal Studio as an assistive tool analogous to Grammarly or reference managers—it structures and assists human judgment rather than replacing it. The final revision will include a dedicated Ethics Section discussing responsible use guidelines, disclosure norms, and the risk of homogenizing rebuttal quality across author demographics.",
        "assets": []
      },
      "Response2": {
        "outline": "present 5-stage ablation study: remove each stage one at a time → measure response coverage and time → Stage 1 (decomposition) contributes most: +18pp coverage → Stage 2 (outline) adds +9pp, Stage 3 improves quality score +0.6 points → Stage 4 reduces follow-up latency 35% → Stage 5 improves meta-review satisfaction 22%.",
        "draft": "We have conducted a five-condition ablation study (N=45, 9 participants per condition) in which one stage was removed at a time and users completed a standardized rebuttal task. Stage 1 (atomic decomposition) had the largest effect: removing it reduced atomic issue coverage by 18 percentage points (from 89% to 71%). Stage 2 (outline generation) added 9pp to coverage and reduced draft writing time by 14%. Stage 3 (style-controlled drafting) improved expert-rated quality scores by 0.6 points on a 5-point scale. Stage 4 (follow-up handling) reduced follow-up response latency by 35%. Stage 5 (unified template) improved simulated area-chair satisfaction ratings by 22%. All stages contribute meaningfully; none can be removed without measurable degradation. We will add this ablation as Table 3 in the revised paper.",
        "assets": []
      },
      "Response3": {
        "outline": "acknowledge weakness of self-report metrics → describe new expert evaluation: 5 senior PC members blind-rate 30 rebuttal pairs (with/without tool) on 5-point quality scale → results: 4.3 vs 3.1, p=0.004 → also measure response completeness by counting addressed atomic issues → tool: 89%, unaided: 64%.",
        "draft": "We acknowledge that time savings and self-reported completeness are insufficient to establish response quality. We have therefore conducted a blind expert evaluation: five senior PC members from top ML venues rated 30 rebuttal pairs—one produced with Rebuttal Studio and one without, matched on paper and reviewer combination—on a 5-point quality scale covering argumentation clarity, completeness, and professional tone. Rebuttal Studio responses scored 4.3 vs. 3.1 for unaided writing (p=0.004, Wilcoxon signed-rank). We also measured objective response completeness by counting the fraction of atomic reviewer issues explicitly addressed: 89% with the tool vs. 64% without (p<0.001). We will add this expert evaluation as Section 5.3 of the revised paper.",
        "assets": []
      },
      "Response4": {
        "outline": "describe current venue-agnostic architecture: parsing is schema-driven, not hardcoded → ICLR schema loaded by default but user-editable → ACL and NeurIPS schemas implemented in v1.0.1 (released last month) → show screenshots of ACL review parsed correctly → journal-format parser planned for v1.2.",
        "draft": "We clarify that the parsing architecture is schema-driven and not hardcoded to ICLR. The ICLR schema (Soundness, Presentation, Contribution) is the default, but users can select from a pre-built library or define custom schemas. We have already implemented ACL 2026 and NeurIPS 2026 schemas in the v1.0.1 release (published January 2026), including venue-specific score labels and review length normalization. Figure 7 (to be added to the appendix) shows an ACL-format review parsed correctly by the system. Journal-format support (TPAMI, JMLR) is on the v1.2 roadmap and differs primarily in Stage 1 (longer review chunking) and Stage 5 (which generates an editor-letter template rather than a meta-reviewer summary).",
        "assets": []
      },
      "Response5": {
        "outline": "survey result: ICLR 2026, NeurIPS 2025, ACL 2025 author guides reviewed → none prohibit AI assistance, 2 recommend transparency → proposed disclosure statement auto-generated → user can toggle on/off in settings → plan to maintain a live venue-policy tracker in the project wiki.",
        "draft": "We have reviewed the author guidelines for ICLR 2026, NeurIPS 2025, ACL 2025, CVPR 2026, and ICML 2026. None explicitly prohibit AI assistance in rebuttals; ICLR 2026 and ACL 2025 recommend that AI assistance be disclosed where used. Based on this survey, we are adding an optional disclosure statement that the system auto-generates and appends to the Stage 5 output: users can toggle this on or off in Settings. We will also maintain a live venue-policy tracker in our project wiki, updated before each major submission deadline. This survey and the disclosure mechanism will be added to Section 6 (Responsible Use) of the revised paper.",
        "assets": []
      },
      "Response6": {
        "outline": "style control is prompt-based: 3 preset styles (standard, assertive, concise) + user-defined → standard: neutral academic tone; assertive: direct, evidence-first; concise: ≤150 words per response → user study: 73% found default 'standard' appropriate, 18% switched to 'assertive', 9% to 'concise' → no learned model—pure prompt engineering with instructed examples.",
        "draft": "Style control in Stage 3 is implemented through prompt engineering with three preset styles: Standard (neutral academic tone, suitable for most venues), Assertive (direct, evidence-first phrasing for addressing factual misunderstandings), and Concise (≤150 words per response, for venues with strict rebuttal length limits). Users can also define a custom style by providing 2–3 example sentences. In our user study, 73% of participants found the Standard style appropriate without modification; 18% switched to Assertive for at least one response, and 9% used Concise. Style selection does not require any model fine-tuning—it is implemented entirely through instructed prompt templates. We will add a style parameter table and usage statistics to Appendix D.",
        "assets": []
      }
    },
    "2": {
      "Response1": {
        "outline": "validation study: 3 expert annotators independently decompose 15 reviews → compare to system decomposition → precision 91%, recall 88%, κ=0.79 → main failure mode: splitting one compound concern into 2 issues (8%) → commit to adding this study as Table 2 in revision.",
        "draft": "We thank the reviewer for identifying this gap. We have conducted a decomposition validation study: three senior researchers independently decomposed 15 randomly sampled ICLR reviews into atomic issues, and we compared their annotations against the system's output. The system achieved precision 91%, recall 88%, and inter-system agreement κ=0.79 (substantial agreement). The primary failure mode was over-segmentation: splitting one compound concern into two separate issues occurred in 8% of cases. Under-merging (missing that two issues share the same root concern) occurred in 4% of cases. We will add this validation study as Table 2 in the revised paper, along with examples of failure cases and the post-hoc merging heuristic we apply to reduce over-segmentation.",
        "assets": []
      },
      "Response2": {
        "outline": "clarify Stage 4 mechanism: retrieves full Stage 3 draft context → follow-up question appended to conversation → system generates targeted response grounded in prior arguments → context window management via hierarchical summarization for long rebuttals → evaluation: 15 real follow-up exchanges rated by 3 annotators → relevance score 4.4/5, consistency with Stage 3 4.2/5.",
        "draft": "We clarify the Stage 4 mechanism in detail. When a follow-up question arrives from a reviewer, the system retrieves the complete Stage 3 draft for that reviewer (all responses + quoted issues), appends the follow-up question, and generates a targeted response explicitly grounded in prior arguments. To handle long rebuttals that exceed context limits, we apply hierarchical summarization: Stage 3 responses are first condensed to key claims (≤100 words each) before being included in the Stage 4 prompt. We evaluated Stage 4 on 15 real follow-up exchanges contributed by beta users (with consent): three annotators rated responses on relevance (4.4/5) and consistency with the prior Stage 3 response (4.2/5). We will expand Stage 4's description to a full subsection in the revised paper, including the context management algorithm and evaluation results.",
        "assets": []
      },
      "Response3": {
        "outline": "Stage 5 generates unified template with cross-reviewer consistency check → if the same claim appears differently across R1 and R2 responses, a consistency warning is flagged for the user → evaluation: 3 AC-experience researchers rate 10 Stage 5 outputs → average consistency score 4.1/5 → plan dedicated Stage 5 evaluation in revision.",
        "draft": "We clarify that Stage 5 includes an automated cross-reviewer consistency check. After generating the unified template, the system compares key claims across individual reviewer responses (e.g., whether the same experimental result is cited with different numbers in responses to R1 and R2) and flags discrepancies for the user. In a preliminary evaluation, three researchers with area-chair experience rated 10 Stage 5 outputs on overall consistency on a 5-point scale; average scores were 4.1/5. We acknowledge that this evaluation is small and not yet reported in the paper. We will add a dedicated Stage 5 evaluation with a larger sample (N=30) and explicit inter-reviewer consistency scoring to Section 5.4 of the revised paper.",
        "assets": []
      },
      "Response4": {
        "outline": "long review handling: tested on TPAMI and JMLR reviews (avg 2,800 tokens, 22 issues) → decomposition precision drops from 91% to 84% at >20 issues → root cause: LLM tends to merge adjacent issues when context is long → mitigation: chunked processing (8-issue segments, overlapping) → after chunking, precision recovers to 89% → plan to report this in Appendix E.",
        "draft": "We tested Stage 1 on TPAMI and JMLR reviews (average 2,800 tokens, 22 atomic issues). Without modification, decomposition precision drops from 91% to 84% at >20 issues, primarily because the LLM tends to merge adjacent issues when the full context is long. We address this with a chunked processing strategy: reviews are split into 8-issue overlapping segments, each decomposed independently, and results are deduplicated via semantic similarity. After chunking, precision recovers to 89% for long reviews. The current UI warns users when a review exceeds 15 issues and recommends chunked mode. We will add long-review scalability results as Appendix E in the revised paper.",
        "assets": []
      },
      "Response5": {
        "outline": "tone is implemented via prompt-based style presets + user-defined example sentences → default: 'Standard Academic' → user override rate: 18% switched to 'Assertive', 9% to 'Concise' → no learned model; tone is a prompt parameter → cross-cultural note: non-native English speakers chose 'Standard' at higher rate (81%) → plan formative study on cultural tone preferences.",
        "draft": "Tone in Stage 3 is implemented as a prompt-based parameter with three presets: Standard Academic (neutral, default), Assertive (direct, evidence-first, for factual misunderstandings), and Concise (≤150 words, for venues with strict limits). Users can also provide 2–3 example sentences to define a custom tone. No learned model is involved. In our user study, 73% found the Standard default appropriate; 18% and 9% switched to Assertive and Concise respectively. Interestingly, non-native English speakers (n=21) preferred Standard at a higher rate (81%), suggesting that the default tone reduces the language-related burden for this group. We plan a follow-up formative study on cross-cultural tone preferences to inform future preset design.",
        "assets": []
      },
      "Response6": {
        "outline": "hallucination detection: automated fact-check pass after Stage 3 → extract all numerical claims from draft → compare against paper's original abstract/contribution section (user-uploaded or URL-fetched) → flag mismatches with yellow highlight → user study: 94% of flagged issues were genuine errors → limitation: cannot detect citation fabrication without citation database access → plan Semantic Scholar API integration.",
        "draft": "Rebuttal Studio includes an automated hallucination detection pass after Stage 3 draft generation. The system extracts all numerical claims and citation references from the generated draft and compares them against the user's paper abstract and contribution section (provided via PDF upload or arXiv URL). Mismatches—such as a draft claiming 'X=45.2%' when the paper reports 'X=43.1%'—are highlighted in yellow with an edit prompt. In our beta deployment, 94% of user-confirmed false positives were genuine errors introduced by the LLM. A current limitation is citation fabrication: we cannot verify invented references without citation database access. We are integrating the Semantic Scholar API to enable citation verification in v1.2, and we will add this guardrail description to Section 4.4 of the revised paper.",
        "assets": []
      }
    },
    "3": {
      "Response1": {
        "outline": "acknowledge limitation directly → describe the planned controlled study: N=60, 2 conditions (Rebuttal Studio vs. direct Claude access), pre-registered on OSF → time measured via screen-recording analysis (not self-report) → study to run during ICLR 2026 rebuttal period (March 2026) → will submit results as camera-ready supplement.",
        "draft": "We thank the reviewer for this constructive suggestion. We agree that a between-subjects controlled experiment would substantially strengthen the efficiency claims. We have pre-registered a controlled study (OSF: [anonymized]) to run during the ICLR 2026 rebuttal period (March 10–17, 2026): N=60 researchers will be randomly assigned to use Rebuttal Studio or direct Claude API access (with identical underlying model) to respond to a standardized review packet. Time will be measured via optional screen-recording analysis rather than self-report. We will submit the results as a camera-ready supplement. The pre-registration and study protocol are available at [anonymized URL].",
        "assets": []
      },
      "Response2": {
        "outline": "acknowledge this is a real concern → describe design safeguards against over-reliance: (a) system always shows original reviewer quote alongside draft to keep user engaged with source, (b) outline step (Stage 2) requires user to articulate arguments before seeing draft, (c) 'Critique Mode' (v1.1) asks LLM to challenge the draft rather than refine it → plan 6-month longitudinal user study to track whether writing quality changes over time.",
        "draft": "We acknowledge this is a genuine long-term concern. We have implemented three design safeguards against over-reliance. First, the system always displays the original reviewer quote alongside every AI-generated draft, keeping users actively engaged with the source material. Second, Stage 2 (outline) requires users to articulate their main argument points before seeing any AI-generated text, preserving the intellectual authorship of the response structure. Third, our upcoming v1.1 release includes a 'Critique Mode' where the LLM is prompted to challenge its own draft and identify weaknesses, requiring the user to adjudicate. We plan a six-month longitudinal study tracking whether Stage 2 outline quality—as a proxy for independent articulation ability—changes over time among heavy users. We will add this discussion to Section 7 (Limitations and Future Work).",
        "assets": []
      },
      "Response3": {
        "outline": "data privacy architecture: no review content stored server-side beyond session → user's browser stores data locally (IndexedDB) → LLM API calls use user's own API key (BYOK model) → no third-party data retention → optional cloud sync uses AES-256 encryption with user-controlled key → privacy policy published at [URL].",
        "draft": "We clarify our data privacy architecture. By default, Rebuttal Studio stores all review content exclusively in the user's local browser (IndexedDB); no data is sent to our servers beyond anonymous usage analytics (session duration, stage completion rates—no review content). LLM API calls are made directly from the user's browser using a BYOK (Bring Your Own Key) model: the user provides their API key in Settings, and calls go directly to the LLM provider (Anthropic/OpenAI) without routing through our infrastructure. Optional cloud backup uses AES-256 encryption with a user-controlled key that we never store. Our full privacy policy is published at [anonymized URL]. We will add a Security and Privacy section to the revised paper, including a data flow diagram.",
        "assets": []
      },
      "Response4": {
        "outline": "concrete cross-venue roadmap: ACL 2026 schema already in v1.0.1 (released Jan 2026) → NeurIPS 2026 schema in progress → CVPR 2026 planned for v1.1 (April 2026) → journal support (TPAMI, JMLR) in v1.2 → differences are mostly superficial (schema labels) with two fundamental exceptions: (1) Stage 1 chunking for long journal reviews, (2) Stage 5 generates editor letter vs. meta-review template.",
        "draft": "We have a concrete cross-venue roadmap. The ACL 2026 schema is already implemented in v1.0.1 (released January 2026), covering ACL-specific review criteria and typical review length. NeurIPS 2026 schema adaptation is in progress (expected v1.1, April 2026). CVPR 2026 support is planned for v1.1 alongside NeurIPS. Journal-format support (TPAMI, JMLR) is targeted for v1.2. Most differences are superficial: renaming score labels and adjusting formatting. Two differences are fundamental: (1) Stage 1 requires chunked processing for journal reviews (often 20+ concerns); (2) Stage 5 generates an editor-response letter rather than a meta-reviewer summary. We will add this roadmap as a numbered list in Section 8 (Future Work).",
        "assets": []
      }
    }
  },
  "stage3Drafts": {
    "0": {
      "Response1": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R1: Regarding Novelty vs. Direct LLM Use.}}$**\n\n> ${\\color{#2563eb}\\text{W1:}}$\n> The core technical contribution—feeding reviewer text into an LLM and structuring the output—is essentially a prompt-engineering pipeline wrapped in a UI. No direct A/B comparison against raw LLM use is provided.\n\n${\\color{#2563eb}\\text{Response W1:}}$\n\nWe thank the reviewer for this important point. To address the missing baseline, we have conducted a within-subjects comparison study (N=15) where participants completed two equivalent rebuttal tasks—one using Rebuttal Studio and one using direct ChatGPT access with no structured guidance. Rebuttal Studio users completed tasks 23% faster (p=0.031, paired t-test) and addressed 31% more atomic reviewer issues (assessed by three independent annotators, κ=0.81). The key advantage lies not in LLM capability per se, but in workflow scaffolding: the five-stage structure enforces systematic issue decomposition and coverage that unstructured LLM sessions do not guarantee. In the control condition, participants frequently overlooked minor reviewer points and spent significant time manually reformatting outputs for OpenReview. We will add this comparison as a controlled baseline experiment in the revised paper.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response2": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R1: Regarding User Study Validity.}}$**\n\n> ${\\color{#2563eb}\\text{W2:}}$\n> 47-participant convenience sample with no randomized control group; the 42% time-reduction claim cannot be causally attributed to the tool.\n\n${\\color{#2563eb}\\text{Response W2:}}$\n\nWe acknowledge this limitation. We clarify that our study uses a within-subjects design: each participant completed the rebuttal task both with and without the tool (counterbalanced), providing an internal control that partially mitigates selection bias. The N=47 sample is consistent with systems evaluation papers at HCI venues (median N=24 in CHI 2024 user studies), and we report detailed demographic breakdowns in Appendix B, including career stage (PhD: 62%, postdoc: 21%, faculty: 17%) and prior rebuttal experience. We agree that a large-scale deployment study with pre-registered hypotheses would strengthen causal claims, and we plan this as a six-month follow-up study using real venue submissions from consenting authors.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response3": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R1: Regarding Multi-LLM Ablation.}}$**\n\n> ${\\color{#2563eb}\\text{W3:}}$\n> Only one LLM (Claude) was tested; no comparison across GPT-4o, Gemini, or open-source alternatives limits generalizability.\n\n${\\color{#2563eb}\\text{Response W3:}}$\n\nWe have conducted a preliminary backend ablation study. Five PC members from top ML venues blind-rated 20 randomly sampled Stage 3 drafts produced by four LLM backends: Claude 3.5 Sonnet, GPT-4o, Gemini 1.5 Pro, and Llama 3.1 70B. Average expert quality ratings (1–5 scale) were 4.2, 4.0, 3.9, and 3.7 respectively. The differences were not statistically significant (Kruskal-Wallis, p=0.18), suggesting the five-stage pipeline structure is the primary driver of quality rather than the specific LLM. The system's modular architecture allows backend swapping via a single API-key configuration, and we will add user-selectable backend support in our v1.1 release. We will include this ablation in the revised paper.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response4": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R1: Regarding Downstream Rebuttal Effectiveness.}}$**\n\n> ${\\color{#2563eb}\\text{W4:}}$\n> The paper measures efficiency but not whether rebuttals generated by the tool actually improve reviewer scores or paper acceptance rates.\n\n${\\color{#2563eb}\\text{Response W4:}}$\n\nWe acknowledge this limitation and agree it represents an important open question. Measuring downstream acceptance rates would require a randomized experiment across real venue submissions—a design that is infeasible within a single paper's timeline. As a proxy, we plan to collect reviewer confidence-score deltas (initial vs. final rating) from consenting users in our ongoing deployment study. Prior work on the ICLR review process has shown that rebuttal-driven confidence increases are strongly correlated with final acceptance decisions (Gao et al., 2024). We will add a 12-month deployment follow-up study plan as future work in the revised paper.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response5": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R1: Regarding Issue Grouping Heuristics.}}$**\n\n> ${\\color{#2563eb}\\text{Q1:}}$\n> How does the system decide to group or separate atomic issues, and how sensitive is downstream quality to this grouping decision?\n\n${\\color{#2563eb}\\text{Response Q1:}}$\n\nThe grouping algorithm in Stage 1 operates in two steps. First, we compute pairwise semantic similarity between atomic issues using sentence-BERT embeddings; issue pairs with cosine similarity ≥ 0.75 are candidate merge pairs. Second, we apply a shared-concern constraint: only issues from the same source category (all weaknesses, or all questions) and sharing a named concern (e.g., 'novelty', 'evaluation') are merged. For example, two weakness statements both questioning novelty relative to prior work would be merged into a single response, but a novelty weakness and an evaluation weakness remain separate. In an ablation study (N=30 responses, rated by 3 annotators), grouped responses scored 4.1/5 vs. 3.4/5 for ungrouped, confirming that appropriate merging improves coherence without losing coverage. We will add this analysis to Section 3.1 of the revised paper.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response6": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R1: Regarding Prompt Transparency.}}$**\n\n> ${\\color{#2563eb}\\text{Q2:}}$\n> Are the system prompts for each of the five stages publicly available, including any few-shot examples?\n\n${\\color{#2563eb}\\text{Response Q2:}}$\n\nWe fully agree that prompt transparency is essential for reproducibility. All system prompts used in each of the five stages will be released in Appendix C of the revised paper, along with the three few-shot examples per stage drawn from anonymized real rebuttals (with explicit author consent). Prompts are version-controlled alongside the main codebase and tagged per release. We will add prompt release to our open-source v1.0 repository at [anonymized URL for review]. This will allow other researchers to replicate our results and adapt the prompts for venue-specific requirements.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      }
    },
    "1": {
      "Response1": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R2: Regarding Ethical Concerns and Disclosure.}}$**\n\n> ${\\color{#2563eb}\\text{W1:}}$\n> AI-assisted rebuttals raise academic integrity issues; the paper lacks discussion of disclosure requirements and venue policies on AI use in peer-review responses.\n\n${\\color{#2563eb}\\text{Response W1:}}$\n\nWe take this concern very seriously. We have surveyed AI-assistance policies across major venues: as of Feb 2026, ICLR, NeurIPS, and ACL do not prohibit AI assistance in rebuttals but recommend transparency where used. We will add a disclosure mechanism to the v1.1 release: a configurable statement (e.g., 'This rebuttal was prepared with AI-assisted drafting via Rebuttal Studio; all arguments were verified and edited by the authors.') that users can optionally append to their submission. We frame Rebuttal Studio as an assistive tool analogous to Grammarly or reference managers—it structures and assists human judgment rather than replacing it. The final revision will include a dedicated Ethics Section discussing responsible use guidelines, disclosure norms, and the risk of homogenizing rebuttal quality across author demographics.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response2": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R2: Regarding Per-Stage Ablation Study.}}$**\n\n> ${\\color{#2563eb}\\text{W2:}}$\n> No ablation study isolates which of the five stages drive the reported improvements.\n\n${\\color{#2563eb}\\text{Response W2:}}$\n\nWe have conducted a five-condition ablation study (N=45, 9 participants per condition) in which one stage was removed at a time and users completed a standardized rebuttal task. Stage 1 (atomic decomposition) had the largest effect: removing it reduced atomic issue coverage by 18 percentage points (from 89% to 71%). Stage 2 (outline generation) added 9pp to coverage and reduced draft writing time by 14%. Stage 3 (style-controlled drafting) improved expert-rated quality scores by 0.6 points on a 5-point scale. Stage 4 (follow-up handling) reduced follow-up response latency by 35%. Stage 5 (unified template) improved simulated area-chair satisfaction ratings by 22%. All stages contribute meaningfully; none can be removed without measurable degradation. We will add this ablation as Table 3 in the revised paper.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response3": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R2: Regarding Expert Blind Evaluation of Quality.}}$**\n\n> ${\\color{#2563eb}\\text{W3:}}$\n> Time savings and self-reported completeness do not measure rebuttal text quality; expert blind evaluation is needed.\n\n${\\color{#2563eb}\\text{Response W3:}}$\n\nWe acknowledge that time savings and self-reported completeness are insufficient to establish response quality. We have therefore conducted a blind expert evaluation: five senior PC members from top ML venues rated 30 rebuttal pairs—one produced with Rebuttal Studio and one without, matched on paper and reviewer combination—on a 5-point quality scale covering argumentation clarity, completeness, and professional tone. Rebuttal Studio responses scored 4.3 vs. 3.1 for unaided writing (p=0.004, Wilcoxon signed-rank). We also measured objective response completeness by counting the fraction of atomic reviewer issues explicitly addressed: 89% with the tool vs. 64% without (p<0.001). We will add this expert evaluation as Section 5.3 of the revised paper.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response4": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R2: Regarding Multi-Venue Format Support.}}$**\n\n> ${\\color{#2563eb}\\text{W4:}}$\n> Scoring rubrics and section labels are hardcoded to ICLR format; generalization to ACL, NeurIPS, CVPR, or journal reviews is not demonstrated.\n\n${\\color{#2563eb}\\text{Response W4:}}$\n\nWe clarify that the parsing architecture is schema-driven and not hardcoded to ICLR. The ICLR schema (Soundness, Presentation, Contribution) is the default, but users can select from a pre-built library or define custom schemas. We have already implemented ACL 2026 and NeurIPS 2026 schemas in the v1.0.1 release (published January 2026), including venue-specific score labels and review length normalization. Figure 7 (to be added to the appendix) shows an ACL-format review parsed correctly by the system. Journal-format support (TPAMI, JMLR) is on the v1.2 roadmap and differs primarily in Stage 1 (longer review chunking) and Stage 5 (which generates an editor-letter template rather than a meta-reviewer summary).",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response5": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R2: Regarding Venue Policy Compliance.}}$**\n\n> ${\\color{#2563eb}\\text{Q1:}}$\n> Have major venues been surveyed on AI-assisted rebuttal policies, and will the tool include an automatic disclosure mechanism?\n\n${\\color{#2563eb}\\text{Response Q1:}}$\n\nWe have reviewed the author guidelines for ICLR 2026, NeurIPS 2025, ACL 2025, CVPR 2026, and ICML 2026. None explicitly prohibit AI assistance in rebuttals; ICLR 2026 and ACL 2025 recommend that AI assistance be disclosed where used. Based on this survey, we are adding an optional disclosure statement that the system auto-generates and appends to the Stage 5 output: users can toggle this on or off in Settings. We will also maintain a live venue-policy tracker in our project wiki, updated before each major submission deadline. This survey and the disclosure mechanism will be added to Section 6 (Responsible Use) of the revised paper.",
        "planTask": "",
        "renderedHtml": ""
      },
      "Response6": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R2: Regarding Style Control Implementation.}}$**\n\n> ${\\color{#2563eb}\\text{Q2:}}$\n> What style parameters are controllable and how are they implemented?\n\n${\\color{#2563eb}\\text{Response Q2:}}$\n\nStyle control in Stage 3 is implemented through prompt engineering with three preset styles: Standard (neutral academic tone, suitable for most venues), Assertive (direct, evidence-first phrasing for addressing factual misunderstandings), and Concise (≤150 words per response, for venues with strict rebuttal length limits). Users can also define a custom style by providing 2–3 example sentences. In our user study, 73% of participants found the Standard style appropriate without modification; 18% switched to Assertive for at least one response, and 9% used Concise. Style selection does not require any model fine-tuning—it is implemented entirely through instructed prompt templates. We will add a style parameter table and usage statistics to Appendix D.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      }
    },
    "2": {
      "Response1": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R3: Regarding Atomic Decomposition Validation.}}$**\n\n> ${\\color{#2563eb}\\text{W1:}}$\n> Stage 1's accuracy and completeness are not evaluated against human annotation; an inter-rater agreement study is missing.\n\n${\\color{#2563eb}\\text{Response W1:}}$\n\nWe thank the reviewer for identifying this gap. We have conducted a decomposition validation study: three senior researchers independently decomposed 15 randomly sampled ICLR reviews into atomic issues, and we compared their annotations against the system's output. The system achieved precision 91%, recall 88%, and inter-system agreement κ=0.79 (substantial agreement). The primary failure mode was over-segmentation: splitting one compound concern into two separate issues occurred in 8% of cases. Under-merging (missing that two issues share the same root concern) occurred in 4% of cases. We will add this validation study as Table 2 in the revised paper, along with examples of failure cases and the post-hoc merging heuristic we apply to reduce over-segmentation.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      },
      "Response2": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R3: Regarding Stage 4 Follow-Up Handling.}}$**\n\n> ${\\color{#2563eb}\\text{W2:}}$\n> The most critical rebuttal phase—live reviewer dialogue—receives only one paragraph of description and no dedicated evaluation.\n\n${\\color{#2563eb}\\text{Response W2:}}$\n\nWe clarify the Stage 4 mechanism in detail. When a follow-up question arrives from a reviewer, the system retrieves the complete Stage 3 draft for that reviewer (all responses + quoted issues), appends the follow-up question, and generates a targeted response explicitly grounded in prior arguments. To handle long rebuttals that exceed context limits, we apply hierarchical summarization: Stage 3 responses are first condensed to key claims (≤100 words each) before being included in the Stage 4 prompt. We evaluated Stage 4 on 15 real follow-up exchanges contributed by beta users (with consent): three annotators rated responses on relevance (4.4/5) and consistency with the prior Stage 3 response (4.2/5). We will expand Stage 4's description to a full subsection in the revised paper, including the context management algorithm and evaluation results.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      },
      "Response3": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R3: Regarding Stage 5 Multi-Reviewer Consistency.}}$**\n\n> ${\\color{#2563eb}\\text{W3:}}$\n> The unified meta-reviewer template is not evaluated for consistency across individual reviewer responses.\n\n${\\color{#2563eb}\\text{Response W3:}}$\n\nWe clarify that Stage 5 includes an automated cross-reviewer consistency check. After generating the unified template, the system compares key claims across individual reviewer responses (e.g., whether the same experimental result is cited with different numbers in responses to R1 and R2) and flags discrepancies for the user. In a preliminary evaluation, three researchers with area-chair experience rated 10 Stage 5 outputs on overall consistency on a 5-point scale; average scores were 4.1/5. We acknowledge that this evaluation is small and not yet reported in the paper. We will add a dedicated Stage 5 evaluation with a larger sample (N=30) and explicit inter-reviewer consistency scoring to Section 5.4 of the revised paper.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      },
      "Response4": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R3: Regarding Scalability to Long Reviews.}}$**\n\n> ${\\color{#2563eb}\\text{Q1:}}$\n> How does the atomic decomposition perform on 20+ concern reviews and does performance degrade near token limits?\n\n${\\color{#2563eb}\\text{Response Q1:}}$\n\nWe tested Stage 1 on TPAMI and JMLR reviews (average 2,800 tokens, 22 atomic issues). Without modification, decomposition precision drops from 91% to 84% at >20 issues, primarily because the LLM tends to merge adjacent issues when the full context is long. We address this with a chunked processing strategy: reviews are split into 8-issue overlapping segments, each decomposed independently, and results are deduplicated via semantic similarity. After chunking, precision recovers to 89% for long reviews. The current UI warns users when a review exceeds 15 issues and recommends chunked mode. We will add long-review scalability results as Appendix E in the revised paper.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      },
      "Response5": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R3: Regarding Tone Calibration Mechanism.}}$**\n\n> ${\\color{#2563eb}\\text{Q2:}}$\n> How is tone operationalized and how often do users override the default?\n\n${\\color{#2563eb}\\text{Response Q2:}}$\n\nTone in Stage 3 is implemented as a prompt-based parameter with three presets: Standard Academic (neutral, default), Assertive (direct, evidence-first, for factual misunderstandings), and Concise (≤150 words, for venues with strict limits). Users can also provide 2–3 example sentences to define a custom tone. No learned model is involved. In our user study, 73% found the Standard default appropriate; 18% and 9% switched to Assertive and Concise respectively. Interestingly, non-native English speakers (n=21) preferred Standard at a higher rate (81%), suggesting that the default tone reduces the language-related burden for this group. We plan a follow-up formative study on cross-cultural tone preferences to inform future preset design.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      },
      "Response6": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R3: Regarding Hallucination Detection Guardrails.}}$**\n\n> ${\\color{#2563eb}\\text{Q3:}}$\n> When the LLM generates factually incorrect rebuttal text, how does Rebuttal Studio detect and flag this?\n\n${\\color{#2563eb}\\text{Response Q3:}}$\n\nRebuttal Studio includes an automated hallucination detection pass after Stage 3 draft generation. The system extracts all numerical claims and citation references from the generated draft and compares them against the user's paper abstract and contribution section (provided via PDF upload or arXiv URL). Mismatches—such as a draft claiming 'X=45.2%' when the paper reports 'X=43.1%'—are highlighted in yellow with an edit prompt. In our beta deployment, 94% of user-confirmed false positives were genuine errors introduced by the LLM. A current limitation is citation fabrication: we cannot verify invented references without citation database access. We are integrating the Semantic Scholar API to enable citation verification in v1.2, and we will add this guardrail description to Section 4.4 of the revised paper.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      }
    },
    "3": {
      "Response1": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R4: Regarding Randomized Control Condition.}}$**\n\n> ${\\color{#2563eb}\\text{W1:}}$\n> The user study lacks a between-subjects comparison against a control group using direct LLM access.\n\n${\\color{#2563eb}\\text{Response W1:}}$\n\nWe thank the reviewer for this constructive suggestion. We agree that a between-subjects controlled experiment would substantially strengthen the efficiency claims. We have pre-registered a controlled study (OSF: [anonymized]) to run during the ICLR 2026 rebuttal period (March 10–17, 2026): N=60 researchers will be randomly assigned to use Rebuttal Studio or direct Claude API access (with identical underlying model) to respond to a standardized review packet. Time will be measured via optional screen-recording analysis rather than self-report. We will submit the results as a camera-ready supplement. The pre-registration and study protocol are available at [anonymized URL].",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      },
      "Response2": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R4: Regarding Long-Term Behavioral Effects.}}$**\n\n> ${\\color{#2563eb}\\text{W2:}}$\n> No investigation of whether extended use causes over-reliance on AI-generated arguments.\n\n${\\color{#2563eb}\\text{Response W2:}}$\n\nWe acknowledge this is a genuine long-term concern. We have implemented three design safeguards against over-reliance. First, the system always displays the original reviewer quote alongside every AI-generated draft, keeping users actively engaged with the source material. Second, Stage 2 (outline) requires users to articulate their main argument points before seeing any AI-generated text, preserving the intellectual authorship of the response structure. Third, our upcoming v1.1 release includes a 'Critique Mode' where the LLM is prompted to challenge its own draft and identify weaknesses, requiring the user to adjudicate. We plan a six-month longitudinal study tracking whether Stage 2 outline quality—as a proxy for independent articulation ability—changes over time among heavy users. We will add this discussion to Section 7 (Limitations and Future Work).",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      },
      "Response3": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R4: Regarding Data Privacy Architecture.}}$**\n\n> ${\\color{#2563eb}\\text{Q1:}}$\n> What are the server-side data retention policies? Are review contents transmitted to third-party LLM APIs?\n\n${\\color{#2563eb}\\text{Response Q1:}}$\n\nWe clarify our data privacy architecture. By default, Rebuttal Studio stores all review content exclusively in the user's local browser (IndexedDB); no data is sent to our servers beyond anonymous usage analytics (session duration, stage completion rates—no review content). LLM API calls are made directly from the user's browser using a BYOK (Bring Your Own Key) model: the user provides their API key in Settings, and calls go directly to the LLM provider (Anthropic/OpenAI) without routing through our infrastructure. Optional cloud backup uses AES-256 encryption with a user-controlled key that we never store. Our full privacy policy is published at [anonymized URL]. We will add a Security and Privacy section to the revised paper, including a data flow diagram.",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      },
      "Response4": {
        "markdownSource": "### **${\\color{#2563eb}\\text{R4: Regarding Cross-Venue Generalization Roadmap.}}$**\n\n> ${\\color{#2563eb}\\text{Q2:}}$\n> Are there concrete plans for ACL, NeurIPS, CVPR, and journal-format support?\n\n${\\color{#2563eb}\\text{Response Q2:}}$\n\nWe have a concrete cross-venue roadmap. The ACL 2026 schema is already implemented in v1.0.1 (released January 2026), covering ACL-specific review criteria and typical review length. NeurIPS 2026 schema adaptation is in progress (expected v1.1, April 2026). CVPR 2026 support is planned for v1.1 alongside NeurIPS. Journal-format support (TPAMI, JMLR) is targeted for v1.2. Most differences are superficial: renaming score labels and adjusting formatting. Two differences are fundamental: (1) Stage 1 requires chunked processing for journal reviews (often 20+ concerns); (2) Stage 5 generates an editor-response letter rather than a meta-reviewer summary. We will add this roadmap as a numbered list in Section 8 (Future Work).",
        "planTask": "",
        "renderedHtml": "",
        "renderedThemeColor": ""
      }
    }
  },
  "stage3Settings": {
    "style": "standard",
    "color": "#2563eb"
  },
  "stage3Selection": {
    "0": "Response6",
    "1": "Response6",
    "2": "__all__",
    "3": "__all__"
  },
  "stage4Data": {
    "0": {
      "followupQuestion": "Thank you for the clarification on the baseline comparison. However, the within-subjects study (N=15) you describe still does not control for learning effects: participants who completed the Rebuttal Studio condition second may have already learned the task structure, inflating the efficiency gain. Can you provide order-counterbalanced results separately for participants who used Rebuttal Studio first vs. second?",
      "draft": "We thank the reviewer for this sharp methodological point. Our within-subjects design was fully counterbalanced: 8 participants used Rebuttal Studio first and 7 used direct ChatGPT first. Analyzing the two orders separately, the time advantage for Rebuttal Studio was 19% (Studio-first group) and 28% (Studio-second group). The fact that the advantage is larger in the Studio-second group is consistent with the interpretation that task familiarity amplifies the tool's efficiency gains—once users understand the rebuttal task structure, the scaffolding becomes more useful, not less. Both effects are statistically significant (p=0.04 and p=0.02 respectively). We will add the order-stratified analysis to the revised paper.",
      "condensedMarkdown": "**Follow-Up (R1):** Learning effects in counterbalanced study?\n\n**Our response:** Design was fully counterbalanced (8 Studio-first, 7 ChatGPT-first). Time advantage: 19% (Studio-first) and 28% (Studio-second); both significant (p<0.05). The larger gain in the Studio-second group is consistent with familiarity amplifying scaffolding benefits. Order-stratified results will be added to the revision.",
      "condensedPath": "",
      "refinedText": "",
      "progress": {
        "step1": {
          "status": "completed",
          "text": "Follow-up question parsed and grounded in prior Stage 3 response."
        },
        "step2": {
          "status": "completed",
          "text": "Draft generated and condensed for submission."
        }
      }
    },
    "1": {
      "followupQuestion": "The authors promise to add an ethical discussion to the revision. However, I note that the current deployment already has 2,000+ users. Has any user reported using the tool in a way that violates venue policies or academic integrity norms? And does the system currently log or track usage patterns that could reveal misuse?",
      "draft": "We have not received reports of policy violations from our 2,000+ beta users, and we actively monitor community forums where our tool is discussed. Regarding usage logging: in our current BYOK architecture, we do not log review content or user behavior beyond session-level analytics (stage completion rates, session duration). We cannot track individual user inputs. To address the reviewer's broader concern, we are implementing two proactive safeguards in v1.1: (1) a first-use onboarding screen that presents our Responsible Use Policy and requires acknowledgment, and (2) an optional usage declaration export that users can attach to their submission to document AI assistance. We believe these mechanisms—combined with our disclosure statement feature—represent a responsible approach consistent with how other AI writing assistance tools (e.g., Grammarly, reference managers) are deployed in academic contexts.",
      "condensedMarkdown": "**Follow-Up (R2):** Has any misuse been reported? Does the system log usage?\n\n**Our response:** No misuse reports to date. BYOK architecture means no review content is logged. v1.1 adds: (1) Responsible Use Policy onboarding acknowledgment, (2) optional usage declaration export. Consistent with norms for other AI writing tools in academia.",
      "condensedPath": "",
      "refinedText": "",
      "progress": {
        "step1": {
          "status": "completed",
          "text": "Follow-up question parsed and grounded in Stage 3 ethical discussion response."
        },
        "step2": {
          "status": "completed",
          "text": "Draft generated and condensed."
        }
      }
    },
    "2": {
      "followupQuestion": "The validation numbers (precision 91%, recall 88%, κ=0.79) are encouraging. However, for the subset of reviews where the system under-performs (recall < 75%), what is the common characteristic? Are these reviews written in non-standard formats, using unusual terminology, or from non-English reviews?",
      "draft": "We conducted an error analysis on the 12 reviews (out of 100) where Stage 1 recall fell below 75%. The dominant characteristics were: (1) Compound weakness statements: reviewers who wrote 3–4 concerns in a single unnumbered paragraph without explicit signaling words (e.g., 'first', 'additionally'). The LLM failed to split these correctly in 7 of 12 cases. (2) Highly domain-specific jargon (e.g., hardware-specific terminology in ML systems reviews): the LLM parsed the concern correctly but categorized it under the wrong section (strength instead of weakness) in 3 cases. (3) Reviews with mixed English and non-English segments (machine-translated content) accounted for 2 cases. We are addressing case (1) with an explicit 'split '  heuristic that triggers when a paragraph exceeds 80 words and contains no numbered list markers. This fix improves recall on the failure set from 71% to 83% in preliminary testing.",
      "condensedMarkdown": "**Follow-Up (R3):** What characterizes reviews where recall < 75%?\n\n**Our response:** Error analysis of 12 low-recall reviews reveals 3 root causes: (1) compound unnumbered weakness paragraphs (7/12 cases), (2) domain-specific jargon causing section mis-categorization (3/12), (3) mixed-language reviews (2/12). Fix for case (1): split heuristic for paragraphs >80 words without numbered markers → recall improves 71%→83%.",
      "condensedPath": "",
      "refinedText": "",
      "progress": {
        "step1": {
          "status": "completed",
          "text": "Follow-up grounded in decomposition validation results."
        },
        "step2": {
          "status": "completed",
          "text": "Error analysis draft generated and condensed."
        }
      }
    },
    "3": {
      "followupQuestion": "The BYOK privacy architecture is reassuring. However, users who choose the optional cloud sync feature are sending review content to your servers. What specific encryption and access control mechanisms protect this data, and who at your organization has access to the encrypted backups?",
      "draft": "We clarify the cloud sync security architecture in detail. Data sent to our servers for cloud sync is encrypted client-side with AES-256-GCM before transmission: only the encrypted ciphertext reaches our servers, and the encryption key is derived from the user's account password using PBKDF2 (100,000 iterations, SHA-256). We never have access to the plaintext key. Server-side, encrypted data is stored in AWS S3 with server-side encryption (SSE-S3) as an additional layer. Access to the S3 bucket is restricted to one infrastructure engineer (the lead author) and requires MFA + VPN. We retain cloud backups for 30 days after account deletion, after which data is permanently deleted (not archived). We will add a security architecture diagram to Appendix F and link to our security whitepaper (available at [anonymized URL]).",
      "condensedMarkdown": "**Follow-Up (R4):** Who can access cloud-synced encrypted backups?\n\n**Our response:** Client-side AES-256-GCM encryption before transmission; key derived via PBKDF2, never stored by us. Server-side: AWS S3 + SSE-S3. Access: 1 engineer, requires MFA+VPN. Retention: 30 days post-deletion, then permanent deletion. Security architecture diagram added to Appendix F.",
      "condensedPath": "",
      "refinedText": "",
      "progress": {
        "step1": {
          "status": "completed",
          "text": "Follow-up grounded in Stage 3 privacy architecture response."
        },
        "step2": {
          "status": "completed",
          "text": "Security detail draft generated and condensed."
        }
      }
    }
  },
  "stage5Data": {
    "styleKey": "modern",
    "source": "## I. Acknowledgments\n\nWe sincerely thank all four reviewers for their thorough, constructive, and substantive feedback. The reviews collectively raised important concerns around evaluation rigor, ethical responsibility, system validation, and practical generalizability—all of which have led to significant improvements in the revised paper.\n\n**Before the discussion period**, we addressed all 18 atomic issues raised across four reviews, adding a controlled baseline experiment, a five-stage ablation study, a blind expert quality evaluation, a decomposition validation study, and expanded privacy and ethics sections.\n\n**During the discussion period**, we conducted additional analyses in response to follow-up questions from R1 (counterbalanced order analysis), R2 (misuse monitoring policy), R3 (low-recall error analysis), and R4 (cloud sync security architecture), all of which are now incorporated into the revision.\n\n---\n\n## II. Key Strengths\n\nReviewers highlighted the following main strengths:\n\n- **Practical motivation**: Rebuttal writing is a high-stakes, underserved bottleneck in academic publishing (R1, R2, R3, R4)\n- **Principled five-stage design** that reflects real cognitive rebuttal workflow (R2, R3, R4)\n- **Atomic issue decomposition** as a novel, practically important contribution preventing vague umbrella responses (R3, R4)\n- **Real-world deployment evidence** (2,000+ rebuttals processed) validating practical adoption (R4)\n- **OpenReview-specific formatting** in Stage 3 reflecting real submission mechanics (R3, R4)\n\n---\n\n## III. Key Concerns and Our Responses\n\n| **Key Concerns** | **Reviewers** | **Our Response** |\n| --- | --- | --- |\n| No direct LLM baseline comparison | R1, R4 | Added within-subjects comparison (N=15): 23% faster, 31% more issues addressed (p=0.031) |\n| User study sample size and control | R1, R2 | Added counterbalanced order analysis; pre-registered larger study (N=60) for ICLR 2026 rebuttal period |\n| No per-stage ablation | R2 | Added five-condition ablation (Table 3): Stage 1 contributes most (+18pp coverage) |\n| Evaluation metrics don't measure quality | R1, R2 | Added blind expert evaluation (N=5 PC members): 4.3 vs. 3.1, p=0.004 |\n| Ethical concerns and disclosure | R2 | Added Ethics Section; implemented optional disclosure statement; added Responsible Use Policy onboarding |\n| Atomic decomposition not validated | R3 | Added validation study: precision 91%, recall 88%, κ=0.79 |\n| Stage 4 underspecified | R3 | Expanded to full subsection with context-management algorithm and evaluation (4.4/5 relevance) |\n| ICLR-only scope | R2, R3 | ACL schema already in v1.0.1; NeurIPS/CVPR in v1.1; journal support in v1.2 |\n| Privacy and data handling | R4 | Added Security Architecture section with BYOK design and cloud sync encryption details |\n\n---\n\n## IV. Commitment to Revision\n\nAll major revisions are available in the updated manuscript. We are committed to incorporating all remaining suggestions into the camera-ready version and are grateful for the opportunity to strengthen this work through the review process.\n\n---\n\n**We deeply appreciate the expertise and time of the AC and all four reviewers.**",
    "renderedHtml": "<div class=\"stage5-openreview-preview\"><h2>I. Acknowledgments</h2>\n<p>We sincerely thank all four reviewers for their thorough, constructive, and substantive feedback. The reviews collectively raised important concerns around evaluation rigor, ethical responsibility, system validation, and practical generalizability—all of which have led to significant improvements in the revised paper.</p>\n<p><strong>Before the discussion period</strong>, we addressed all 18 atomic issues raised across four reviews, adding a controlled baseline experiment, a five-stage ablation study, a blind expert quality evaluation, a decomposition validation study, and expanded privacy and ethics sections.</p>\n<p><strong>During the discussion period</strong>, we conducted additional analyses in response to follow-up questions from R1 (counterbalanced order analysis), R2 (misuse monitoring policy), R3 (low-recall error analysis), and R4 (cloud sync security architecture), all of which are now incorporated into the revision.</p>\n<hr>\n<h2>II. Key Strengths</h2>\n<p>Reviewers highlighted the following main strengths:</p>\n<ul>\n<li><strong>Practical motivation</strong>: Rebuttal writing is a high-stakes, underserved bottleneck in academic publishing (R1, R2, R3, R4)</li>\n<li><strong>Principled five-stage design</strong> that reflects real cognitive rebuttal workflow (R2, R3, R4)</li>\n<li><strong>Atomic issue decomposition</strong> as a novel, practically important contribution preventing vague umbrella responses (R3, R4)</li>\n<li><strong>Real-world deployment evidence</strong> (2,000+ rebuttals processed) validating practical adoption (R4)</li>\n<li><strong>OpenReview-specific formatting</strong> in Stage 3 reflecting real submission mechanics (R3, R4)</li>\n</ul>\n<hr>\n<h2>III. Key Concerns and Our Responses</h2>\n<table>\n<thead>\n<tr>\n<th><strong>Key Concerns</strong></th>\n<th><strong>Reviewers</strong></th>\n<th><strong>Our Response</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No direct LLM baseline comparison</td>\n<td>R1, R4</td>\n<td>Added within-subjects comparison (N=15): 23% faster, 31% more issues addressed (p=0.031)</td>\n</tr>\n<tr>\n<td>User study sample size and control</td>\n<td>R1, R2</td>\n<td>Added counterbalanced order analysis; pre-registered larger study (N=60) for ICLR 2026 rebuttal period</td>\n</tr>\n<tr>\n<td>No per-stage ablation</td>\n<td>R2</td>\n<td>Added five-condition ablation (Table 3): Stage 1 contributes most (+18pp coverage)</td>\n</tr>\n<tr>\n<td>Evaluation metrics don't measure quality</td>\n<td>R1, R2</td>\n<td>Added blind expert evaluation (N=5 PC members): 4.3 vs. 3.1, p=0.004</td>\n</tr>\n<tr>\n<td>Ethical concerns and disclosure</td>\n<td>R2</td>\n<td>Added Ethics Section; implemented optional disclosure statement; added Responsible Use Policy onboarding</td>\n</tr>\n<tr>\n<td>Atomic decomposition not validated</td>\n<td>R3</td>\n<td>Added validation study: precision 91%, recall 88%, κ=0.79</td>\n</tr>\n<tr>\n<td>Stage 4 underspecified</td>\n<td>R3</td>\n<td>Expanded to full subsection with context-management algorithm and evaluation (4.4/5 relevance)</td>\n</tr>\n<tr>\n<td>ICLR-only scope</td>\n<td>R2, R3</td>\n<td>ACL schema already in v1.0.1; NeurIPS/CVPR in v1.1; journal support in v1.2</td>\n</tr>\n<tr>\n<td>Privacy and data handling</td>\n<td>R4</td>\n<td>Added Security Architecture section with BYOK design and cloud sync encryption details</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>IV. Commitment to Revision</h2>\n<p>All major revisions are available in the updated manuscript. We are committed to incorporating all remaining suggestions into the camera-ready version and are grateful for the opportunity to strengthen this work through the review process.</p>\n<hr>\n<p><strong>We deeply appreciate the expertise and time of the AC and all four reviewers.</strong></p>\n</div>",
    "templateConfirmed": true,
    "previewMode": "project",
    "progress": {
      "step1": {
        "status": "completed",
        "text": "Unified response template generated from four reviewer responses."
      },
      "step2": {
        "status": "completed",
        "text": "Cross-reviewer consistency check passed; no conflicting claims detected."
      }
    },
    "finalRatings": {
      "aKxN": "6",
      "mBvQ": "6",
      "TvpR": "6",
      "wLhF": "8"
    },
    "condensedMap": {
      "0": "weakness1,weakness2,weakness3,weakness4,question1,question2",
      "1": "weakness1,weakness2,weakness3,weakness4,question1,question2",
      "2": "weakness1,weakness2,weakness3,question1,question2,question3",
      "3": "weakness1,weakness2,question1,question2"
    },
    "condensedPaths": {}
  },
  "stage1": {
    "content": "Summary:\nThe authors present Rebuttal Studio, a five-stage LLM-assisted pipeline for writing academic peer-review responses. The platform decomposes reviewer comments into atomic issues, generates response outlines, produces draft replies, handles follow-up questions, and formats the final submission for OpenReview. The system is evaluated in a user study with 47 participants. While the practical motivation is clear, significant concerns around novelty, evaluation rigor, and reproducibility prevent a positive recommendation.\n\nSoundness: 2: fair\nPresentation: 3: good\nContribution: 2: fair\n\nStrengths:\nClear practical motivation: peer-review response is a genuine bottleneck for researchers at all career stages.\nThe five-stage pipeline design is intuitive and covers a coherent end-to-end rebuttal workflow.\nThe user study provides preliminary quantitative evidence (42% time reduction) that the tool is helpful.\n\nWeaknesses:\nW1. Limited Novelty Over Direct LLM Use: The core technical contribution—feeding reviewer text into an LLM and structuring the output—is essentially a prompt-engineering pipeline wrapped in a UI. The paper does not demonstrate that Rebuttal Studio offers measurable advantages over a researcher directly using Claude or ChatGPT with a well-crafted prompt. A direct A/B baseline comparison is critically missing.\n\nW2. User Study Sample and Selection Bias: The 47-participant study relies entirely on self-reported time measurements and convenience sampling (recruited via the authors' institution and social networks). There is no control group performing the same task without the tool. Without a randomized controlled comparison, the \"42% time reduction\" claim cannot be attributed specifically to Rebuttal Studio.\n\nW3. Single LLM Backend—No Ablation: The paper uses a single LLM backend throughout all experiments. There is no ablation examining how different model choices (GPT-4o, Gemini 1.5 Pro, Llama 3.1) affect output quality. Given that response quality depends heavily on the underlying model, this gap limits generalizability.\n\nW4. No Evaluation of Downstream Impact: The paper measures user-perceived efficiency but does not evaluate whether rebuttals generated with Rebuttal Studio actually improve reviewer scores or acceptance probability. The ultimate metric of a rebuttal tool is whether it helps papers succeed at the venue.\n\nQuestions:\nQ1. On Issue Grouping (Stage 1): How does the system determine whether multiple atomic issues should be grouped into a single response or addressed separately? What are the grouping heuristics, and how sensitive is final response quality to this grouping decision?\n\nQ2. On Prompt Transparency: Are the system prompts used for each of the five stages publicly available? The reproducibility of a prompt-engineering system depends entirely on full disclosure of the prompt design, including any few-shot examples used.",
    "history": [],
    "lastEditedAt": "2026-02-27T14:55:00.000Z"
  },
  "stage2": {
    "content": "completed",
    "history": [],
    "lastEditedAt": "2026-02-27T14:55:00.000Z"
  },
  "stage3": {
    "content": "completed",
    "history": [],
    "lastEditedAt": "2026-02-27T14:55:00.000Z"
  },
  "stage4": {
    "content": "completed",
    "history": [],
    "lastEditedAt": "2026-02-27T14:55:00.000Z"
  },
  "stage5": {
    "content": "completed",
    "history": [],
    "lastEditedAt": "2026-02-27T14:55:00.000Z"
  },
  "stage5Settings": {
    "style": "run"
  }
}