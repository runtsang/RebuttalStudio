{
  "projectName": "TokenSeek",
  "conference": "ICLR",
  "createdAt": "2026-02-21T23:30:18.195Z",
  "updatedAt": "2026-02-22T01:10:15.902Z",
  "autosaveIntervalSeconds": 60,
  "currentStage": "Reply",
  "stage1": {
    "content": "Summary:\n\nThis paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.\n\nSoundness: 3: good\nPresentation: 3: good\nContribution: 2: fair\nStrengths:\nThe paper is clearly written and generally easy to follow, with only minor typos.\nThe proposed TokenSeek method is conceptually simple and practically implementable.\nExperimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.\nThe experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.\nWeaknesses:\n\nLimited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.\n\nComparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.\n\nMissing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.\n\nSome minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models\n\n[2] Sparse Matrix in Large Language Model Fine-tuning\n\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\nQuestions:\n\nCode-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.\n\nSource of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.\n\nAt present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.\n\nFlag For Ethics Review: No ethics review needed.\nRating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted\nConfidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct: Yes",
    "history": [
      {
        "timestamp": "2026-02-21T23:30:41.025Z",
        "content": "<div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">2: fair</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The paper is clearly written and generally easy to follow, with only minor typos.</li><li style=\"padding: 0px;\">The proposed TokenSeek method is conceptually simple and practically implementable.</li><li style=\"padding: 0px;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</li><li style=\"padding: 0px;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</li></ul></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</p><p style=\"margin: 0px 0px 0.5rem;\">[2] Sparse Matrix in Large Language Model Fine-tuning</p><p style=\"margin: 0px 0px 0.5rem;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Flag For Ethics Review:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">No ethics review needed.</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Code Of Conduct:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">Yes</span></div>"
      }
    ],
    "lastEditedAt": "2026-02-22T00:32:02.006Z"
  },
  "stage2": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage3": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage4": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage5": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "reviewers": [
    {
      "id": 0,
      "name": "ZpqT",
      "content": "<div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: good</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">2: fair</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The paper is clearly written and generally easy to follow, with only minor typos.</li><li style=\"padding: 0px;\">The proposed TokenSeek method is conceptually simple and practically implementable.</li><li style=\"padding: 0px;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</li><li style=\"padding: 0px;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</li></ul></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</p><p style=\"margin: 0px 0px 0.5rem;\">[2] Sparse Matrix in Large Language Model Fine-tuning</p><p style=\"margin: 0px 0px 0.5rem;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</p></div></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Flag For Ethics Review:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">No ethics review needed.</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></div><div style=\"color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 12px; white-space-collapse: collapse; background-color: rgb(247, 246, 244);\"><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Code Of Conduct:</span>&nbsp;<span class=\"note-content-value\" style=\"white-space-collapse: preserve; overflow-wrap: break-word;\">Yes</span></div>"
    }
  ],
  "breakdownData": {
    "0": {
      "scores": {
        "rating": "4",
        "confidence": "3",
        "soundness": "3",
        "presentation": "3",
        "contribution": "2"
      },
      "sections": {
        "summary": "This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.",
        "strength": "The paper is clearly written and generally easy to follow, with only minor typos.\nThe proposed TokenSeek method is conceptually simple and practically implementable.\nExperimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.\nThe experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.",
        "weakness": "Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.\n\nComparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.\n\nMissing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.\n\nSome minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve",
        "questions": "Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.\n\nSource of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
      },
      "atomicIssues": [
        {
          "id": "weakness1",
          "source": "weakness",
          "text": "Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements."
        },
        {
          "id": "weakness2",
          "source": "weakness",
          "text": "Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches."
        },
        {
          "id": "weakness3",
          "source": "weakness",
          "text": "Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution."
        },
        {
          "id": "weakness4",
          "source": "weakness",
          "text": "Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve"
        },
        {
          "id": "question1",
          "source": "question",
          "text": "Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability."
        },
        {
          "id": "question2",
          "source": "question",
          "text": "Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
        }
      ],
      "responses": [
        {
          "id": "Response1",
          "title": "Novelty relative to TokenTune",
          "source": "weakness",
          "source_id": "weakness1",
          "quoted_issue": "Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements."
        },
        {
          "id": "Response2",
          "title": "Comparison with Low-Rank/Partial-Tuning",
          "source": "weakness",
          "source_id": "weakness2",
          "quoted_issue": "Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches."
        },
        {
          "id": "Response3",
          "title": "Missing Baselines",
          "source": "weakness",
          "source_id": "weakness3",
          "quoted_issue": "Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution."
        },
        {
          "id": "Response4",
          "title": "Minor Typos",
          "source": "weakness",
          "source_id": "weakness4",
          "quoted_issue": "Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve"
        },
        {
          "id": "Response5",
          "title": "Code-Domain Generalization",
          "source": "question",
          "source_id": "question1",
          "quoted_issue": "Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability."
        },
        {
          "id": "Response6",
          "title": "Mechanism of Memory Savings",
          "source": "question",
          "source_id": "question2",
          "quoted_issue": "Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
        }
      ]
    }
  }
}