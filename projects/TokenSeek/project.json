{
  "projectName": "TokenSeek",
  "conference": "ICLR",
  "createdAt": "2026-02-22T01:38:14.264Z",
  "updatedAt": "2026-02-22T06:11:36.277Z",
  "autosaveIntervalSeconds": 60,
  "currentStage": "Reply",
  "stage2Replies": {
    "0": {
      "Response1": {
        "outline": "motivation difference: engineering perspective → reduce memory via partial gradients / data-agnostic selection → ineffective and unstable across tasks; unequal token contribution observation → data-driven, instance-aware design → effective and stable.\n\nexperiment setting difference: memory reduction validation under various settings → demonstrate memory saving effectiveness; memory + performance evaluation → add stability + interpretability + optimization analysis → reveal token-level contribution in efficient training.\n\nnovelty in meft: prior methods rarely use token-level, instance-aware selection → introduce instance-aware token selection for activation-memory–efficient training → core novelty distinguishing tokenseek from tokentune.",
        "draft": "",
        "assets": []
      },
      "Response2": {
        "outline": "memory reduction: data-driven salient token identification → discard substantially more activations → maintain or even improve performance under the same setting → achieve significantly lower memory at equal accuracy.\n\ngeneralizability: rely only on inherent pretrained signals (attention + gradients) → no auxiliary model, no extra training, no architectural modification → compatible with any transformer-based model and peft method → unlike customized designs (e.g., reversible networks), remain universally plug-and-play across architectures and domains.",
        "draft": "",
        "assets": []
      },
      "Response3": {
        "outline": "baseline clarification: training speed advantage of lora → memory efficiency and stability advantage of tokenseek → emphasize trade-off beyond speed.\n\nempirical comparison under qwen setting: lower memory usage (19.2% vs. 81.2%) → higher accuracy (48.45 vs. 48.06) → demonstrate simultaneous memory and performance gains.\n\ndegradation evidence: tokentune shows clear performance drop (46.34 vs. 48.06) → highlight robustness advantage of tokenseek → revise manuscript to clarify these distinctions.",
        "draft": "",
        "assets": []
      },
      "Response4": {
        "outline": "acknowledge suggestion → agree on need for more diverse and comprehensive baselines → update related work to include additional references.\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models. arxiv 2024.\n[2] Sparse Matrix in Large Language Model Fine-tuning. arxiv 2024.\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. ICLR 2019.\n[4] RandLora: Full-rank parameter-efficient fine-tuning of large models. ICLR 2025.\n\nprior baseline coverage → included boft as representative sparsity-based peft method → following suggestion, add ranlora experiments under qwen2.5 (0.5b) setting → expand empirical comparison.\n\n| Method                | Ave. Mem. | Max. Mem. | ARC   | HellaSwag | MMLU  | TruthfulQA | WinoGrande | Average Score |\n| --------------------- | --------- | --------- | ----- | --------- | ----- | ---------- | ---------- | ------------- |\n| BOFT                  | 145.1%    | 100.6%    | 34.64 | 51.70     | 58.18 | 39.57      | 56.43      | 48.10         |\n| RanLoRA               | 95.4%     | 86.7%     | 29.18 | 50.10     | 58.33 | 45.21      | 57.22      | 48.01         |\n| QLoRA                 | 51.7%     | 45.6%     | 34.64 | 50.10     | 58.05 | 40.41      | 55.09      | 47.66         |\n| - w/ TokenSeek (Ours) | 19.2%     | 13.4%     | 34.56 | 50.09     | 57.52 | 41.51      | 58.56      | **48.45**     |\n\nimpact of revision → improve comprehensiveness of evaluation → further demonstrate effectiveness under requested setting → update manuscript with new results and discussion.",
        "draft": "",
        "assets": []
      },
      "Response5": {
        "outline": "appreciate the reviewer’s careful reading and feedback.\nrevise the manuscript to improve clarity and precision accordingly.",
        "draft": "",
        "assets": []
      },
      "Response6": {
        "outline": "acknowledge constructive suggestion → agree on importance of coding task evaluation → conduct preliminary experiments under llama3.2 (1b) setting.\n\n| Method                  | Ave. Mem. | Max. Mem. | ARC   | HellaSwag | MMLU  | TruthfulQA | WinoGrande | Humaneval | Average Score |\n| ----------------------- | --------- | --------- | ----- | --------- | ----- | ---------- | ---------- | --------- | ------------- |\n| **LoHa**                | 92.3%     | 99.4%     | 39.25 | 65.93     | 57.60 | 37.87      | 60.77      | 13.41     | 45.81         |\n| – w/ TokenTune (Random) | 45.9%     | 28.4%     | 38.48 | 64.21     | 50.34 | 43.89      | 59.91      | 10.97     | 44.63         |\n| – w/ TokenSeek (Ours)   | 45.9%     | 28.4%     | 38.57 | 65.89     | 58.18 | 39.34      | 60.93      | 14.02     | 46.16         |\n| **QLoRA**               | 45.6%     | 34.8%     | 38.82 | 65.26     | 56.39 | 38.85      | 61.33      | 14.02     | 45.78         |\n| – w/ TokenTune (Random) | 14.8%     | 14.3%     | 39.33 | 62.97     | 41.76 | 41.36      | 60.69      | 12.80     | 43.15         |\n| – w/ TokenSeek (Ours)   | 14.8%     | 14.3%     | 39.08 | 65.98     | 58.03 | 38.65      | 61.33      | 14.63     | 46.28         |\n\nchallenge: coding tasks contain denser information → concern about effectiveness under higher information density → evaluate tokenseek in this setting.\n\nempirical finding: tokenseek remains effective → attribute to instance-aware token ditching and full forward pass preservation → retain complete attention and contextual information.\n\nrevision update: include new experimental results and corresponding discussion in the manuscript.\n",
        "draft": "",
        "assets": []
      },
      "Response7": {
        "outline": "two stages: offline instance-aware token seeking → online efficient token ditching during training.\n\nscoring design: forward pass for context score + gradient only at penultimate layer (earlier layers frozen) → much lower memory than full fine-tuning → store per-instance token mapping.\n\ntraining mechanism: ditch gradients of unselected tokens based on stored mapping → see section 3.1 and 3.2.2 for theoretical memory analysis → empirical results align with theory.\n\nmemory source summary: partial backward scoring + instance-aware direct token ditching → revise manuscript to clarify and expand discussion.",
        "draft": "",
        "assets": []
      }
    }
  },
  "stage1": {
    "content": "Summary:\n\nThis paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.\n\nSoundness: 3: good\nPresentation: 3: good\nContribution: 2: fair\nStrengths:\nThe paper is clearly written and generally easy to follow, with only minor typos.\nThe proposed TokenSeek method is conceptually simple and practically implementable.\nExperimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.\nThe experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.\nWeaknesses:\n\nLimited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.\n\nComparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.\n\nMissing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.\n\nSome minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models\n\n[2] Sparse Matrix in Large Language Model Fine-tuning\n\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\nQuestions:\n\nCode-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.\n\nSource of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.\n\nAt present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.\n\nFlag For Ethics Review: No ethics review needed.\nRating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted\nConfidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct: Yes",
    "history": [
      {
        "timestamp": "2026-02-22T01:38:29.362Z",
        "content": "<span id=\"docs-internal-guid-6b7215a0-7fff-7f8b-e46d-40d27fc0c30f\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Summary:</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Soundness:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 3: good</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Presentation:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 3: good</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Contribution:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 2: fair</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Strengths:</span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\"><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">The paper is clearly written and generally easy to follow, with only minor typos.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">The proposed TokenSeek method is conceptually simple and practically implementable.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</span></p></li></ul><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Weaknesses:</span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\"><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</span></p></li></ul><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">[2] Sparse Matrix in Large Language Model Fine-tuning</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Questions:</span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\"><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</span></p></li></ul><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Flag For Ethics Review:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> No ethics review needed.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Rating:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Confidence:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Code Of Conduct:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> Yes</span></p><div><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"><br></span></div></span>"
      },
      {
        "timestamp": "2026-02-22T01:38:42.565Z",
        "content": "<span id=\"docs-internal-guid-6b7215a0-7fff-7f8b-e46d-40d27fc0c30f\"><div class=\"note-content-container \" style=\"position: relative;\"><div class=\"note-content\" style=\"font-size: 0.75rem; line-height: 1.125rem; margin: 0px;\"><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">2: fair</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The paper is clearly written and generally easy to follow, with only minor typos.</li><li style=\"padding: 0px;\">The proposed TokenSeek method is conceptually simple and practically implementable.</li><li style=\"padding: 0px;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</li><li style=\"padding: 0px;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</li></ul></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</p><p style=\"margin: 0px 0px 0.5rem;\">[2] Sparse Matrix in Large Language Model Fine-tuning</p><p style=\"margin: 0px 0px 0.5rem;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Flag For Ethics Review:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">No ethics review needed.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Code Of Conduct:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">Yes</span></div></div></div><div class=\"note-replies\" style=\"margin-top: 0.75rem; padding-left: 2.5rem;\"><div class=\"note  depth-even\" data-id=\"KiqRUPqHRX\" style=\"position: relative; padding: 0.5rem; border: 1px solid rgb(238, 238, 238); border-radius: 2px; background-color: rgb(255, 253, 250); margin-bottom: 0.5rem;\"><div class=\"btn-group-vertical btn-group-xs collapse-controls-v\" role=\"group\" aria-label=\"Collapse controls\" style=\"position: absolute; display: inline-block; vertical-align: middle; left: -24px; top: -1px; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div><div class=\"heading\" style=\"display: flex; justify-content: space-between; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div></div></div></span>"
      },
      {
        "timestamp": "2026-02-22T01:38:47.703Z",
        "content": "<span id=\"docs-internal-guid-6b7215a0-7fff-7f8b-e46d-40d27fc0c30f\"><div class=\"note-replies\" style=\"margin-top: 0.75rem; padding-left: 2.5rem;\"><div class=\"note  depth-even\" data-id=\"KiqRUPqHRX\" style=\"position: relative; padding: 0.5rem; border: 1px solid rgb(238, 238, 238); border-radius: 2px; background-color: rgb(255, 253, 250); margin-bottom: 0.5rem;\"><br><div class=\"btn-group-vertical btn-group-xs collapse-controls-v\" role=\"group\" aria-label=\"Collapse controls\" style=\"position: absolute; display: inline-block; vertical-align: middle; left: -24px; top: -1px; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div><div class=\"heading\" style=\"display: flex; justify-content: space-between; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div></div></div></span>"
      },
      {
        "timestamp": "2026-02-22T01:38:47.881Z",
        "content": ""
      },
      {
        "timestamp": "2026-02-22T01:38:49.141Z",
        "content": "<div class=\"note-content-container \" style=\"position: relative;\"><div class=\"note-content\" style=\"font-size: 0.75rem; line-height: 1.125rem; margin: 0px;\"><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">2: fair</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The paper is clearly written and generally easy to follow, with only minor typos.</li><li style=\"padding: 0px;\">The proposed TokenSeek method is conceptually simple and practically implementable.</li><li style=\"padding: 0px;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</li><li style=\"padding: 0px;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</li></ul></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</p><p style=\"margin: 0px 0px 0.5rem;\">[2] Sparse Matrix in Large Language Model Fine-tuning</p><p style=\"margin: 0px 0px 0.5rem;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Flag For Ethics Review:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">No ethics review needed.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Code Of Conduct:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">Yes</span></div></div></div><div class=\"note-replies\" style=\"margin-top: 0.75rem; padding-left: 2.5rem;\"><div class=\"note  depth-even\" data-id=\"KiqRUPqHRX\" style=\"position: relative; padding: 0.5rem; border: 1px solid rgb(238, 238, 238); border-radius: 2px; background-color: rgb(255, 253, 250); margin-bottom: 0.5rem;\"><div class=\"btn-group-vertical btn-group-xs collapse-controls-v\" role=\"group\" aria-label=\"Collapse controls\" style=\"position: absolute; display: inline-block; vertical-align: middle; left: -24px; top: -1px; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div><div class=\"heading\" style=\"display: flex; justify-content: space-between; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div></div></div>"
      }
    ],
    "lastEditedAt": "2026-02-22T01:39:20.175Z"
  },
  "stage2": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage3": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage4": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage5": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "reviewers": [
    {
      "id": 0,
      "name": "ZpqT",
      "content": "<div class=\"note-content-container \" style=\"position: relative;\"><div class=\"note-content\" style=\"font-size: 0.75rem; line-height: 1.125rem; margin: 0px;\"><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">2: fair</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The paper is clearly written and generally easy to follow, with only minor typos.</li><li style=\"padding: 0px;\">The proposed TokenSeek method is conceptually simple and practically implementable.</li><li style=\"padding: 0px;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</li><li style=\"padding: 0px;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</li></ul></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</p><p style=\"margin: 0px 0px 0.5rem;\">[2] Sparse Matrix in Large Language Model Fine-tuning</p><p style=\"margin: 0px 0px 0.5rem;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Flag For Ethics Review:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">No ethics review needed.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Code Of Conduct:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">Yes</span></div></div></div><div class=\"note-replies\" style=\"margin-top: 0.75rem; padding-left: 2.5rem;\"><div class=\"note  depth-even\" data-id=\"KiqRUPqHRX\" style=\"position: relative; padding: 0.5rem; border: 1px solid rgb(238, 238, 238); border-radius: 2px; background-color: rgb(255, 253, 250); margin-bottom: 0.5rem;\"><div class=\"btn-group-vertical btn-group-xs collapse-controls-v\" role=\"group\" aria-label=\"Collapse controls\" style=\"position: absolute; display: inline-block; vertical-align: middle; left: -24px; top: -1px; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div><div class=\"heading\" style=\"display: flex; justify-content: space-between; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div></div></div>"
    }
  ],
  "breakdownData": {
    "0": {
      "scores": {
        "rating": "4",
        "confidence": "3",
        "soundness": "3",
        "presentation": "3",
        "contribution": "2"
      },
      "sections": {
        "summary": "This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.",
        "strength": "The paper is clearly written and generally easy to follow, with only minor typos.\nThe proposed TokenSeek method is conceptually simple and practically implementable.\nExperimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.\nThe experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.",
        "weakness": "Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.\n\nComparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.\n\nMissing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.\n\nSome minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models\n\n[2] Sparse Matrix in Large Language Model Fine-tuning\n\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
        "questions": "Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.\n\nSource of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
      },
      "atomicIssues": [
        {
          "id": "weakness1",
          "source": "weakness",
          "text": "Novelty relative to TokenTune: Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm."
        },
        {
          "id": "weakness2",
          "source": "weakness",
          "text": "Novelty relative to TokenTune (Part 2): While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements."
        },
        {
          "id": "weakness3",
          "source": "weakness",
          "text": "Comparison with Low-Rank/Partial-Tuning: Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches."
        },
        {
          "id": "weakness4",
          "source": "weakness",
          "text": "Missing Sparsity-based PEFT Baselines: Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution."
        },
        {
          "id": "weakness5",
          "source": "weakness",
          "text": "Correction of Typos: Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve"
        },
        {
          "id": "question1",
          "source": "question",
          "text": "Generalization to Code Domains: Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability."
        },
        {
          "id": "question2",
          "source": "question",
          "text": "Mechanism of Activation Memory Reduction: Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
        }
      ],
      "responses": [
        {
          "id": "Response1",
          "title": "Novelty relative to TokenTune",
          "source": "weakness",
          "source_id": "weakness1",
          "quoted_issue": "Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm."
        },
        {
          "title": "Novelty relative to TokenTune (Part 2)",
          "source": "weakness",
          "quoted_issue": "While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.",
          "id": "Response2",
          "source_id": "weakness2"
        },
        {
          "id": "Response3",
          "title": "Comparison with Low-Rank/Partial-Tuning",
          "source": "weakness",
          "source_id": "weakness3",
          "quoted_issue": "Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches."
        },
        {
          "id": "Response4",
          "title": "Missing Sparsity-based PEFT Baselines",
          "source": "weakness",
          "source_id": "weakness4",
          "quoted_issue": "Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution."
        },
        {
          "id": "Response5",
          "title": "Correction of Typos",
          "source": "weakness",
          "source_id": "weakness5",
          "quoted_issue": "Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve"
        },
        {
          "id": "Response6",
          "title": "Generalization to Code Domains",
          "source": "question",
          "source_id": "question1",
          "quoted_issue": "Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability."
        },
        {
          "id": "Response7",
          "title": "Mechanism of Activation Memory Reduction",
          "source": "question",
          "source_id": "question2",
          "quoted_issue": "Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
        }
      ]
    }
  },
  "stage3Settings": {
    "style": "standard",
    "color": "#f26921"
  },
  "stage3Drafts": {
    "0": {
      "Response1": {
        "planTask": ""
      },
      "Response2": {
        "planTask": ""
      },
      "Response3": {
        "planTask": ""
      },
      "Response4": {
        "planTask": ""
      },
      "Response5": {
        "planTask": ""
      },
      "Response6": {
        "planTask": ""
      },
      "Response7": {
        "planTask": ""
      }
    }
  },
  "stage3Selection": {
    "0": "Response1"
  }
}