{
  "projectName": "TokenSeek",
  "conference": "ICLR",
  "createdAt": "2026-02-22T01:38:14.264Z",
  "updatedAt": "2026-02-22T15:26:16.291Z",
  "autosaveIntervalSeconds": 60,
  "currentStage": "First Round",
  "stage2Replies": {
    "0": {
      "Response1": {
        "outline": "motivation difference: engineering perspective → reduce memory via partial gradients / data-agnostic selection → ineffective and unstable across tasks; unequal token contribution observation → data-driven, instance-aware design → effective and stable.\n\nexperiment setting difference: memory reduction validation under various settings → demonstrate memory saving effectiveness; memory + performance evaluation → add stability + interpretability + optimization analysis → reveal token-level contribution in efficient training.\n\nnovelty in meft: prior methods rarely use token-level, instance-aware selection → introduce instance-aware token selection for activation-memory–efficient training → core novelty distinguishing tokenseek from tokentune.",
        "draft": "We thank the reviewer for the opportunity to clarify our novelty relative to TokenTune. While both works address memory efficiency, our approach differs significantly in motivation, methodology, and experimental scope.\n\n**Motivation:** TokenTune primarily adopts an engineering perspective, focusing on memory reduction via partial gradients and data-agnostic selection. However, such methods can be ineffective and unstable across different tasks. In contrast, our work is motivated by the observation of unequal token contributions. We propose a data-driven, instance-aware design that ensures both effectiveness and stability.\n\n**Experimental Scope:** Beyond memory reduction validation, we provide a comprehensive evaluation of performance, stability, and interpretability. Our optimization analysis reveals the underlying token-level contributions, providing deeper insights into efficient training dynamics.\n\n**Novelty in MEFT:** Prior methods in Memory-Efficient Fine-Tuning (MEFT) rarely utilize token-level, instance-aware selection. The introduction of instance-aware selection specifically for activation-memory-efficient training represents a core novelty that distinguishes our method from TokenTune.",
        "assets": []
      },
      "Response2": {
        "outline": "memory reduction: data-driven salient token identification → discard substantially more activations → maintain or even improve performance under the same setting → achieve significantly lower memory at equal accuracy.\n\ngeneralizability: rely only on inherent pretrained signals (attention + gradients) → no auxiliary model, no extra training, no architectural modification → compatible with any transformer-based model and peft method → unlike customized designs (e.g., reversible networks), remain universally plug-and-play across architectures and domains.",
        "draft": "We clarify that TokenSeek introduces fundamental advancements in both memory efficiency and generalizability over TokenTune. Regarding memory reduction, TokenSeek employs a data-driven strategy for salient token identification, enabling the system to discard substantially more activations while maintaining or improving performance. This allows TokenSeek to achieve significantly lower memory overhead at equivalent accuracy levels. In terms of generalizability, TokenSeek relies solely on inherent pretrained signals—specifically attention maps and gradients—eliminating the need for auxiliary models, extra training, or architectural modifications. Unlike customized designs such as reversible networks, TokenSeek is universally plug-and-play, ensuring compatibility across diverse Transformer architectures, domains, and PEFT methods.",
        "assets": []
      },
      "Response3": {
        "outline": "baseline clarification: training speed advantage of lora → memory efficiency and stability advantage of tokenseek → emphasize trade-off beyond speed.\n\nempirical comparison under qwen setting: lower memory usage (19.2% vs. 81.2%) → higher accuracy (48.45 vs. 48.06) → demonstrate simultaneous memory and performance gains.\n\ndegradation evidence: tokentune shows clear performance drop (46.34 vs. 48.06) → highlight robustness advantage of tokenseek → revise manuscript to clarify these distinctions.",
        "draft": "We thank the reviewer for the suggestion to compare TokenSeek with low-rank and partial-tuning methods. While we acknowledge that methods like LoRA offer training speed advantages, TokenSeek provides distinct benefits in memory efficiency and training stability. In our empirical comparison under the Qwen setting, TokenSeek demonstrates significantly lower memory usage (19.2% vs. 81.2%) while simultaneously achieving higher accuracy (48.45 vs. 48.06) compared to standard baselines. Furthermore, while TokenTune exhibits a clear performance drop (46.34 vs. 48.06), TokenSeek maintains robustness and avoids such degradation. We will revise the manuscript to clarify these trade-offs and highlight the non-trivial memory and performance gains offered by TokenSeek.",
        "assets": []
      },
      "Response4": {
        "outline": "acknowledge suggestion → agree on need for more diverse and comprehensive baselines → update related work to include additional references.\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models. arxiv 2024.\n[2] Sparse Matrix in Large Language Model Fine-tuning. arxiv 2024.\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. ICLR 2019.\n[4] RandLora: Full-rank parameter-efficient fine-tuning of large models. ICLR 2025.\n\nprior baseline coverage → included boft as representative sparsity-based peft method → following suggestion, add ranlora experiments under qwen2.5 (0.5b) setting → expand empirical comparison.\n\n| Method                | Ave. Mem. | Max. Mem. | ARC   | HellaSwag | MMLU  | TruthfulQA | WinoGrande | Average Score |\n| --------------------- | --------- | --------- | ----- | --------- | ----- | ---------- | ---------- | ------------- |\n| BOFT                  | 145.1%    | 100.6%    | 34.64 | 51.70     | 58.18 | 39.57      | 56.43      | 48.10         |\n| RanLoRA               | 95.4%     | 86.7%     | 29.18 | 50.10     | 58.33 | 45.21      | 57.22      | 48.01         |\n| QLoRA                 | 51.7%     | 45.6%     | 34.64 | 50.10     | 58.05 | 40.41      | 55.09      | 47.66         |\n| - w/ TokenSeek (Ours) | 19.2%     | 13.4%     | 34.56 | 50.09     | 57.52 | 41.51      | 58.56      | **48.45**     |\n\nimpact of revision → improve comprehensiveness of evaluation → further demonstrate effectiveness under requested setting → update manuscript with new results and discussion.",
        "draft": "We thank the reviewer for the suggestion to include more diverse sparsity-based PEFT baselines. We agree that expanding our comparisons provides a more comprehensive evaluation of TokenSeek's contribution. Accordingly, we have updated our related work to include references [1-4].\n\nIn our initial submission, we included BOFT as a representative sparsity-based PEFT method. To further address the reviewer's comment, we have added experiments using RanLoRA [4] under the Qwen2.5 (0.5B) setting. The updated results are summarized below:\n\n| Method | Ave. Mem. | Max. Mem. | ARC | HellaSwag | MMLU | TruthfulQA | WinoGrande | Average Score |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| BOFT | 145.1% | 100.6% | 34.64 | 51.70 | 58.18 | 39.57 | 56.43 | 48.10 |\n| RanLoRA | 95.4% | 86.7% | 29.18 | 50.10 | 58.33 | 45.21 | 57.22 | 48.01 |\n| QLoRA | 51.7% | 45.6% | 34.64 | 50.10 | 58.05 | 40.41 | 55.09 | 47.66 |\n| - w/ TokenSeek (Ours) | 19.2% | 13.4% | 34.56 | 50.09 | 57.52 | 41.51 | 58.56 | **48.45** |\n\nAs shown, TokenSeek achieves a superior average score of 48.45 while maintaining significantly lower memory overhead (19.2% average and 13.4% maximum memory) compared to BOFT, RanLoRA, and QLoRA. These additions improve the comprehensiveness of our evaluation and further demonstrate TokenSeek's effectiveness. We have updated the manuscript with these new results and the expanded discussion.",
        "assets": []
      },
      "Response5": {
        "outline": "appreciate the reviewer’s careful reading and feedback.\nrevise the manuscript to improve clarity and precision accordingly.",
        "draft": "We thank the reviewer for their careful reading and for pointing out these typographical errors. We have corrected the identified instances, including \"showin\" to \"shown\", \"Benifit\" to \"Benefit\", \"Unde\" to \"Under\", and \"achiving\" to \"achieving\", as well as the subject-verb agreement issues noted. We will perform a thorough revision of the manuscript to improve clarity and linguistic precision.",
        "assets": []
      },
      "Response6": {
        "outline": "acknowledge constructive suggestion → agree on importance of coding task evaluation → conduct preliminary experiments under llama3.2 (1b) setting.\n\n| Method                  | Ave. Mem. | Max. Mem. | ARC   | HellaSwag | MMLU  | TruthfulQA | WinoGrande | Humaneval | Average Score |\n| ----------------------- | --------- | --------- | ----- | --------- | ----- | ---------- | ---------- | --------- | ------------- |\n| **LoHa**                | 92.3%     | 99.4%     | 39.25 | 65.93     | 57.60 | 37.87      | 60.77      | 13.41     | 45.81         |\n| – w/ TokenTune (Random) | 45.9%     | 28.4%     | 38.48 | 64.21     | 50.34 | 43.89      | 59.91      | 10.97     | 44.63         |\n| – w/ TokenSeek (Ours)   | 45.9%     | 28.4%     | 38.57 | 65.89     | 58.18 | 39.34      | 60.93      | 14.02     | 46.16         |\n| **QLoRA**               | 45.6%     | 34.8%     | 38.82 | 65.26     | 56.39 | 38.85      | 61.33      | 14.02     | 45.78         |\n| – w/ TokenTune (Random) | 14.8%     | 14.3%     | 39.33 | 62.97     | 41.76 | 41.36      | 60.69      | 12.80     | 43.15         |\n| – w/ TokenSeek (Ours)   | 14.8%     | 14.3%     | 39.08 | 65.98     | 58.03 | 38.65      | 61.33      | 14.63     | 46.28         |\n\nchallenge: coding tasks contain denser information → concern about effectiveness under higher information density → evaluate tokenseek in this setting.\n\nempirical finding: tokenseek remains effective → attribute to instance-aware token ditching and full forward pass preservation → retain complete attention and contextual information.\n\nrevision update: include new experimental results and corresponding discussion in the manuscript.\n",
        "draft": "We thank the reviewer for the constructive suggestion to evaluate TokenSeek on code-related datasets. We agree that the high information density of code presents a unique challenge for token-sparsity methods. To investigate this, we conducted experiments using Llama 3.2 (1B) on the HumanEval benchmark. Our results indicate that TokenSeek maintains strong performance in the code domain. For instance, with LoHa, TokenSeek achieved a HumanEval score of 14.02 (vs. 13.41 for standard LoHa), significantly outperforming the random selection baseline (TokenTune) at 10.97. Similarly, with QLoRA, TokenSeek reached 14.63 on HumanEval compared to 14.02 for standard QLoRA. These findings suggest that TokenSeek’s instance-aware selection and preservation of the full forward pass effectively retain the necessary contextual information, even in information-dense sequences. We have updated the manuscript to include these experimental results and further discussion on code-domain generalization.",
        "assets": []
      },
      "Response7": {
        "outline": "two stages: offline instance-aware token seeking → online efficient token ditching during training.\n\nscoring design: forward pass for context score + gradient only at penultimate layer (earlier layers frozen) → much lower memory than full fine-tuning → store per-instance token mapping.\n\ntraining mechanism: ditch gradients of unselected tokens based on stored mapping → see section 3.1 and 3.2.2 for theoretical memory analysis → empirical results align with theory.\n\nmemory source summary: partial backward scoring + instance-aware direct token ditching → revise manuscript to clarify and expand discussion.",
        "draft": "We thank the reviewer for the opportunity to clarify the source of memory savings in TokenSeek. The reduction stems from a two-stage process: offline instance-aware token seeking followed by online efficient token ditching during training. In the seeking stage, we compute context scores using a forward pass and gradients only at the penultimate layer, keeping earlier layers frozen. This approach requires significantly less memory than full fine-tuning. We then store the resulting per-instance token mapping. During the training stage, we 'ditch' the gradients of unselected tokens based on this mapping. As detailed in Sections 3.1 and 3.2.2, our theoretical memory analysis shows that activation memory is reduced because we avoid storing and processing gradients for the discarded tokens. Our empirical results align with this analysis. We will revise the manuscript to further clarify these mechanisms and expand the discussion on memory efficiency.",
        "assets": []
      }
    }
  },
  "stage1": {
    "content": "Summary:\n\nThis paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.\n\nSoundness: 3: good\nPresentation: 3: good\nContribution: 2: fair\nStrengths:\nThe paper is clearly written and generally easy to follow, with only minor typos.\nThe proposed TokenSeek method is conceptually simple and practically implementable.\nExperimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.\nThe experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.\nWeaknesses:\n\nLimited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.\n\nComparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.\n\nMissing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.\n\nSome minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models\n\n[2] Sparse Matrix in Large Language Model Fine-tuning\n\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\nQuestions:\n\nCode-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.\n\nSource of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.\n\nAt present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.\n\nFlag For Ethics Review: No ethics review needed.\nRating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted\nConfidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct: Yes",
    "history": [
      {
        "timestamp": "2026-02-22T01:38:29.362Z",
        "content": "<span id=\"docs-internal-guid-6b7215a0-7fff-7f8b-e46d-40d27fc0c30f\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Summary:</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Soundness:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 3: good</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Presentation:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 3: good</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Contribution:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 2: fair</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Strengths:</span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\"><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">The paper is clearly written and generally easy to follow, with only minor typos.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">The proposed TokenSeek method is conceptually simple and practically implementable.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</span></p></li></ul><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Weaknesses:</span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\"><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</span></p></li></ul><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">[2] Sparse Matrix in Large Language Model Fine-tuning</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Questions:</span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\"><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</span></p></li><li dir=\"ltr\" style=\"list-style-type: disc; font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: nowrap;\" aria-level=\"1\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\" role=\"presentation\"><span style=\"font-size: 9pt; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; text-wrap: wrap;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</span></p></li></ul><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Flag For Ethics Review:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> No ethics review needed.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Rating:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 6pt 0pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Confidence:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#f7f6f4;margin-top:0pt;margin-bottom:6pt;\"><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(140, 27, 19); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Code Of Conduct:</span><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> Yes</span></p><div><span style=\"font-size: 9pt; font-family: Arial, sans-serif; color: rgb(51, 51, 51); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"><br></span></div></span>"
      },
      {
        "timestamp": "2026-02-22T01:38:42.565Z",
        "content": "<span id=\"docs-internal-guid-6b7215a0-7fff-7f8b-e46d-40d27fc0c30f\"><div class=\"note-content-container \" style=\"position: relative;\"><div class=\"note-content\" style=\"font-size: 0.75rem; line-height: 1.125rem; margin: 0px;\"><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">2: fair</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The paper is clearly written and generally easy to follow, with only minor typos.</li><li style=\"padding: 0px;\">The proposed TokenSeek method is conceptually simple and practically implementable.</li><li style=\"padding: 0px;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</li><li style=\"padding: 0px;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</li></ul></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</p><p style=\"margin: 0px 0px 0.5rem;\">[2] Sparse Matrix in Large Language Model Fine-tuning</p><p style=\"margin: 0px 0px 0.5rem;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Flag For Ethics Review:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">No ethics review needed.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Code Of Conduct:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">Yes</span></div></div></div><div class=\"note-replies\" style=\"margin-top: 0.75rem; padding-left: 2.5rem;\"><div class=\"note  depth-even\" data-id=\"KiqRUPqHRX\" style=\"position: relative; padding: 0.5rem; border: 1px solid rgb(238, 238, 238); border-radius: 2px; background-color: rgb(255, 253, 250); margin-bottom: 0.5rem;\"><div class=\"btn-group-vertical btn-group-xs collapse-controls-v\" role=\"group\" aria-label=\"Collapse controls\" style=\"position: absolute; display: inline-block; vertical-align: middle; left: -24px; top: -1px; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div><div class=\"heading\" style=\"display: flex; justify-content: space-between; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div></div></div></span>"
      },
      {
        "timestamp": "2026-02-22T01:38:47.703Z",
        "content": "<span id=\"docs-internal-guid-6b7215a0-7fff-7f8b-e46d-40d27fc0c30f\"><div class=\"note-replies\" style=\"margin-top: 0.75rem; padding-left: 2.5rem;\"><div class=\"note  depth-even\" data-id=\"KiqRUPqHRX\" style=\"position: relative; padding: 0.5rem; border: 1px solid rgb(238, 238, 238); border-radius: 2px; background-color: rgb(255, 253, 250); margin-bottom: 0.5rem;\"><br><div class=\"btn-group-vertical btn-group-xs collapse-controls-v\" role=\"group\" aria-label=\"Collapse controls\" style=\"position: absolute; display: inline-block; vertical-align: middle; left: -24px; top: -1px; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div><div class=\"heading\" style=\"display: flex; justify-content: space-between; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div></div></div></span>"
      },
      {
        "timestamp": "2026-02-22T01:38:47.881Z",
        "content": ""
      },
      {
        "timestamp": "2026-02-22T01:38:49.141Z",
        "content": "<div class=\"note-content-container \" style=\"position: relative;\"><div class=\"note-content\" style=\"font-size: 0.75rem; line-height: 1.125rem; margin: 0px;\"><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">2: fair</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The paper is clearly written and generally easy to follow, with only minor typos.</li><li style=\"padding: 0px;\">The proposed TokenSeek method is conceptually simple and practically implementable.</li><li style=\"padding: 0px;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</li><li style=\"padding: 0px;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</li></ul></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</p><p style=\"margin: 0px 0px 0.5rem;\">[2] Sparse Matrix in Large Language Model Fine-tuning</p><p style=\"margin: 0px 0px 0.5rem;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Flag For Ethics Review:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">No ethics review needed.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Code Of Conduct:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">Yes</span></div></div></div><div class=\"note-replies\" style=\"margin-top: 0.75rem; padding-left: 2.5rem;\"><div class=\"note  depth-even\" data-id=\"KiqRUPqHRX\" style=\"position: relative; padding: 0.5rem; border: 1px solid rgb(238, 238, 238); border-radius: 2px; background-color: rgb(255, 253, 250); margin-bottom: 0.5rem;\"><div class=\"btn-group-vertical btn-group-xs collapse-controls-v\" role=\"group\" aria-label=\"Collapse controls\" style=\"position: absolute; display: inline-block; vertical-align: middle; left: -24px; top: -1px; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div><div class=\"heading\" style=\"display: flex; justify-content: space-between; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div></div></div>"
      }
    ],
    "lastEditedAt": "2026-02-22T01:39:20.175Z"
  },
  "stage2": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage3": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage4": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "stage5": {
    "content": "",
    "history": [],
    "lastEditedAt": null
  },
  "reviewers": [
    {
      "id": 0,
      "name": "ZpqT",
      "content": "<div class=\"note-content-container \" style=\"position: relative;\"><div class=\"note-content\" style=\"font-size: 0.75rem; line-height: 1.125rem; margin: 0px;\"><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Summary:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><p style=\"margin: 0px 0px 0.5rem;\">This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Soundness:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Presentation:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: good</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Contribution:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">2: fair</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Strengths:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\">The paper is clearly written and generally easy to follow, with only minor typos.</li><li style=\"padding: 0px;\">The proposed TokenSeek method is conceptually simple and practically implementable.</li><li style=\"padding: 0px;\">Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.</li><li style=\"padding: 0px;\">The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.</li></ul></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Weaknesses:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">[1] Scaling Sparse Fine-Tuning to Large Language Models</p><p style=\"margin: 0px 0px 0.5rem;\">[2] Sparse Matrix in Large Language Model Fine-tuning</p><p style=\"margin: 0px 0px 0.5rem;\">[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Questions:</span><div class=\"note-content-value markdown-rendered\" style=\"white-space-collapse: collapse; overflow-wrap: break-word;\"><ul style=\"margin-top: 0px; margin-bottom: 0.5rem; padding-left: 1.5rem;\"><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.</p></li><li style=\"padding: 0px;\"><p style=\"margin: 0px 0px 0.125rem;\">Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.</p></li></ul><p style=\"margin: 0px 0px 0.5rem;\">At present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score.</p></div></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Flag For Ethics Review:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">No ethics review needed.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Rating:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">4: marginally below the acceptance threshold. But would not mind if paper is accepted</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Confidence:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.</span></div><div><span class=\"note-content-field disable-tex-rendering\" style=\"font-weight: 700; color: rgb(140, 27, 19); padding-right: 0.25rem;\">Code Of Conduct:</span>&nbsp;<span class=\"note-content-value\" style=\"overflow-wrap: break-word;\">Yes</span></div></div></div><div class=\"note-replies\" style=\"margin-top: 0.75rem; padding-left: 2.5rem;\"><div class=\"note  depth-even\" data-id=\"KiqRUPqHRX\" style=\"position: relative; padding: 0.5rem; border: 1px solid rgb(238, 238, 238); border-radius: 2px; background-color: rgb(255, 253, 250); margin-bottom: 0.5rem;\"><div class=\"btn-group-vertical btn-group-xs collapse-controls-v\" role=\"group\" aria-label=\"Collapse controls\" style=\"position: absolute; display: inline-block; vertical-align: middle; left: -24px; top: -1px; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div><div class=\"heading\" style=\"display: flex; justify-content: space-between; color: rgb(51, 51, 51); font-family: &quot;Noto Sans&quot;, &quot;Noto Sans Fallback&quot;; font-size: 14px; white-space-collapse: collapse;\"></div></div></div>"
    }
  ],
  "breakdownData": {
    "0": {
      "scores": {
        "rating": "4",
        "confidence": "3",
        "soundness": "3",
        "presentation": "3",
        "contribution": "2"
      },
      "sections": {
        "summary": "This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance.",
        "strength": "The paper is clearly written and generally easy to follow, with only minor typos.\nThe proposed TokenSeek method is conceptually simple and practically implementable.\nExperimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.\nThe experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility.",
        "weakness": "Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.\n\nComparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.\n\nMissing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.\n\nSome minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models\n\n[2] Sparse Matrix in Large Language Model Fine-tuning\n\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
        "questions": "Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.\n\nSource of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
      },
      "atomicIssues": [
        {
          "id": "weakness1",
          "source": "weakness",
          "text": "Novelty relative to TokenTune: Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm."
        },
        {
          "id": "weakness2",
          "source": "weakness",
          "text": "Novelty relative to TokenTune (Part 2): While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements."
        },
        {
          "id": "weakness3",
          "source": "weakness",
          "text": "Comparison with Low-Rank/Partial-Tuning: Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches."
        },
        {
          "id": "weakness4",
          "source": "weakness",
          "text": "Missing Sparsity-based PEFT Baselines: Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution."
        },
        {
          "id": "weakness5",
          "source": "weakness",
          "text": "Correction of Typos: Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve"
        },
        {
          "id": "question1",
          "source": "question",
          "text": "Generalization to Code Domains: Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability."
        },
        {
          "id": "question2",
          "source": "question",
          "text": "Mechanism of Activation Memory Reduction: Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
        }
      ],
      "responses": [
        {
          "id": "Response1",
          "title": "Novelty relative to TokenTune",
          "source": "weakness",
          "source_id": "weakness1",
          "quoted_issue": "Limited Novelty Clarification: The main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm."
        },
        {
          "title": "Novelty relative to TokenTune (Part 2)",
          "source": "weakness",
          "quoted_issue": "While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.",
          "id": "Response2",
          "source_id": "weakness2"
        },
        {
          "id": "Response3",
          "title": "Comparison with Low-Rank/Partial-Tuning",
          "source": "weakness",
          "source_id": "weakness3",
          "quoted_issue": "Comparison to Low-Rank and Partial-Tuning Methods: On Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches."
        },
        {
          "id": "Response4",
          "title": "Missing Sparsity-based PEFT Baselines",
          "source": "weakness",
          "source_id": "weakness4",
          "quoted_issue": "Missing Baselines and Related Work: Several recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution."
        },
        {
          "id": "Response5",
          "title": "Correction of Typos",
          "source": "weakness",
          "source_id": "weakness5",
          "quoted_issue": "Some minor typos “as showin in Fig. 1 (a)” → shown; “Benifit from” → Benefit from; L313 “Unde” → Under; “achiving” → achieving; “TokenSeek achieve” → achieves; “LoRA/QLoRA achieves” → plural → achieve"
        },
        {
          "id": "Response6",
          "title": "Generalization to Code Domains",
          "source": "question",
          "source_id": "question1",
          "quoted_issue": "Code-Domain Generalization: The experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability."
        },
        {
          "id": "Response7",
          "title": "Mechanism of Activation Memory Reduction",
          "source": "question",
          "source_id": "question2",
          "quoted_issue": "Source of Memory Savings: Please clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness."
        }
      ]
    }
  },
  "stage3Settings": {
    "style": "standard",
    "color": "#f26921"
  },
  "stage3Drafts": {
    "0": {
      "Response1": {
        "planTask": ""
      },
      "Response2": {
        "planTask": ""
      },
      "Response3": {
        "planTask": ""
      },
      "Response4": {
        "planTask": ""
      },
      "Response5": {
        "planTask": ""
      },
      "Response6": {
        "planTask": ""
      },
      "Response7": {
        "planTask": ""
      }
    }
  },
  "stage3Selection": {
    "0": "Response1"
  }
}